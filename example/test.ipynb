{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from scipy.sparse import csgraph\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from scipy import sparse\n",
    "from collections import defaultdict\n",
    "from dataPrepose import *\n",
    "from graphConvolution import *\n",
    "import pymetis\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat,  nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.lr1 = nn.Linear(nfeat, nclass,bias=True)\n",
    "        self.lr2 = nn.Linear(nfeat, nclass,bias=True)\n",
    "        self.lr3 = nn.Linear(nfeat, nclass,bias=True)\n",
    "        self.lr4 = nn.Linear(nfeat, nclass,bias=True)\n",
    "        self.lr5 = nn.Linear(nfeat, nclass,bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj1,adj2, adj3,adj4,adj5):\n",
    "        x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x1 = torch.mm(adj1,x_d)\n",
    "        x1 = self.lr1(x1)\n",
    "        \n",
    "        x2 = torch.mm(adj2,x_d)\n",
    "        x2 = self.lr2(x2)\n",
    "        \n",
    "        x3 = torch.mm(adj3,x_d)\n",
    "        x3 = self.lr1(x3)\n",
    "        \n",
    "        x4 = torch.mm(adj4,x_d)\n",
    "        x4 = self.lr4(x4)\n",
    "        \n",
    "        x5 = torch.mm(adj5,x_d)\n",
    "        x5 = self.lr5(x5)\n",
    "        \n",
    "        x = x1+ x2 +x3+ x4+x5 \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model,record,features,adj1,adj2, adj3,adj4,adj5):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj1,adj2, adj3,adj4,adj5)\n",
    "    loss_train = F.cross_entropy(output[idx_train], labels[idx_train]) \n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    output = model(features, adj1,adj2, adj3,adj4,adj5)\n",
    "\n",
    "    loss_val = F.cross_entropy(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    loss_test = F.cross_entropy(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'acc_test: {:.4f}'.format(acc_test.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    record[acc_val.item()] = acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1]: Upload citeseer dataset.\n",
      "| # of nodes : 3327\n",
      "| # of edges : 4614.0\n",
      "| # of features : 3703\n",
      "| # of clases   : 6\n",
      "| # of train set : 120\n",
      "| # of val set   : 500\n",
      "| # of test set  : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zwt/1.Codes_P2CG/dataPrepose.py:39: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(dataset = 'citeseer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.eye(adj.shape[0])\n",
    "features = torch.FloatTensor(features)\n",
    "features = features.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_matrix(adj.todense())\n",
    "all_edges = list(G.edges())# number of edges\n",
    "random.Random(1).shuffle(all_edges)\n",
    "num_edges = len(all_edges)\n",
    "m = int(num_edges/2)\n",
    "num_nodes = adj.shape[0] #number of nodes\n",
    "A_edges = all_edges[:m]\n",
    "B_edges = all_edges[m:]\n",
    "A_adj = np.zeros((num_nodes,num_nodes))\n",
    "B_adj = np.zeros((num_nodes,num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in A_edges:\n",
    "    A_adj[edge[0]][edge[1]] = 1\n",
    "    A_adj[edge[1]][edge[0]] = 1\n",
    "for edge in B_edges:\n",
    "    B_adj[edge[0]][edge[1]] = 1\n",
    "    B_adj[edge[1]][edge[0]] = 1\n",
    "\n",
    "idx_train = idx_train.cuda()\n",
    "idx_test = idx_test.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "labels = labels.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Centralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_adj_sparse = sparse.csr_matrix(adj)\n",
    "adj1 = normalize_adj(A_adj_sparse + sp.eye(A_adj.shape[0]))\n",
    "adj1 = torch.FloatTensor(adj1.todense())\n",
    "adj1 = adj1.cuda()\n",
    "adj2 = torch.mm(adj1,adj1)\n",
    "adj3 = torch.mm(adj2,adj1)\n",
    "adj4 = torch.mm(adj3,adj1)\n",
    "adj5 = torch.mm(adj4,adj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.7947 acc_train: 0.1417 acc_val: 0.1320 acc_test: 0.1700 time: 0.1121s\n",
      "Epoch: 0002 loss_train: 1.7400 acc_train: 0.3417 acc_val: 0.4300 acc_test: 0.4300 time: 0.1111s\n",
      "Epoch: 0003 loss_train: 1.6679 acc_train: 0.6667 acc_val: 0.4720 acc_test: 0.4460 time: 0.1035s\n",
      "Epoch: 0004 loss_train: 1.6012 acc_train: 0.7417 acc_val: 0.4500 acc_test: 0.4370 time: 0.1026s\n",
      "Epoch: 0005 loss_train: 1.5484 acc_train: 0.7083 acc_val: 0.4800 acc_test: 0.4590 time: 0.1024s\n",
      "Epoch: 0006 loss_train: 1.4741 acc_train: 0.7917 acc_val: 0.4800 acc_test: 0.4690 time: 0.1023s\n",
      "Epoch: 0007 loss_train: 1.3626 acc_train: 0.9000 acc_val: 0.5420 acc_test: 0.5330 time: 0.1022s\n",
      "Epoch: 0008 loss_train: 1.3151 acc_train: 0.9000 acc_val: 0.5240 acc_test: 0.5290 time: 0.1023s\n",
      "Epoch: 0009 loss_train: 1.3056 acc_train: 0.8500 acc_val: 0.4640 acc_test: 0.4600 time: 0.1023s\n",
      "Epoch: 0010 loss_train: 1.2466 acc_train: 0.8667 acc_val: 0.4300 acc_test: 0.4360 time: 0.1024s\n",
      "Epoch: 0011 loss_train: 1.1841 acc_train: 0.8750 acc_val: 0.4480 acc_test: 0.4490 time: 0.1022s\n",
      "Epoch: 0012 loss_train: 1.0454 acc_train: 0.9250 acc_val: 0.4600 acc_test: 0.4420 time: 0.1022s\n",
      "Epoch: 0013 loss_train: 0.9867 acc_train: 0.9083 acc_val: 0.4560 acc_test: 0.4370 time: 0.1022s\n",
      "Epoch: 0014 loss_train: 0.9790 acc_train: 0.8750 acc_val: 0.4440 acc_test: 0.4300 time: 0.1024s\n",
      "Epoch: 0015 loss_train: 0.9710 acc_train: 0.8583 acc_val: 0.4600 acc_test: 0.4260 time: 0.1022s\n",
      "Epoch: 0016 loss_train: 0.9997 acc_train: 0.8333 acc_val: 0.4640 acc_test: 0.4360 time: 0.1023s\n",
      "Epoch: 0017 loss_train: 0.9261 acc_train: 0.8750 acc_val: 0.4980 acc_test: 0.4800 time: 0.1023s\n",
      "Epoch: 0018 loss_train: 0.8065 acc_train: 0.8917 acc_val: 0.4940 acc_test: 0.4860 time: 0.1022s\n",
      "Epoch: 0019 loss_train: 0.8246 acc_train: 0.8667 acc_val: 0.4940 acc_test: 0.4840 time: 0.1023s\n",
      "Epoch: 0020 loss_train: 0.8476 acc_train: 0.8500 acc_val: 0.4960 acc_test: 0.4980 time: 0.1023s\n",
      "Epoch: 0021 loss_train: 0.8033 acc_train: 0.8833 acc_val: 0.5040 acc_test: 0.4940 time: 0.1022s\n",
      "Epoch: 0022 loss_train: 0.8017 acc_train: 0.8750 acc_val: 0.4780 acc_test: 0.4450 time: 0.1024s\n",
      "Epoch: 0023 loss_train: 0.8532 acc_train: 0.8833 acc_val: 0.4780 acc_test: 0.4410 time: 0.1023s\n",
      "Epoch: 0024 loss_train: 0.8284 acc_train: 0.8667 acc_val: 0.4760 acc_test: 0.4370 time: 0.1023s\n",
      "Epoch: 0025 loss_train: 0.7063 acc_train: 0.8583 acc_val: 0.4580 acc_test: 0.4430 time: 0.1022s\n",
      "Epoch: 0026 loss_train: 0.8562 acc_train: 0.8583 acc_val: 0.4580 acc_test: 0.4440 time: 0.1024s\n",
      "Epoch: 0027 loss_train: 0.7349 acc_train: 0.8417 acc_val: 0.4820 acc_test: 0.4780 time: 0.1022s\n",
      "Epoch: 0028 loss_train: 0.7834 acc_train: 0.8417 acc_val: 0.5020 acc_test: 0.5290 time: 0.1024s\n",
      "Epoch: 0029 loss_train: 0.7373 acc_train: 0.9083 acc_val: 0.5060 acc_test: 0.5090 time: 0.1021s\n",
      "Epoch: 0030 loss_train: 0.6541 acc_train: 0.9250 acc_val: 0.5040 acc_test: 0.5060 time: 0.1024s\n",
      "Epoch: 0031 loss_train: 0.8189 acc_train: 0.8750 acc_val: 0.5080 acc_test: 0.5090 time: 0.1022s\n",
      "Epoch: 0032 loss_train: 0.6909 acc_train: 0.8917 acc_val: 0.5140 acc_test: 0.5160 time: 0.1023s\n",
      "Epoch: 0033 loss_train: 0.6914 acc_train: 0.9417 acc_val: 0.4600 acc_test: 0.4190 time: 0.1023s\n",
      "Epoch: 0034 loss_train: 0.7065 acc_train: 0.8833 acc_val: 0.4580 acc_test: 0.4160 time: 0.1024s\n",
      "Epoch: 0035 loss_train: 0.7985 acc_train: 0.8667 acc_val: 0.4580 acc_test: 0.4060 time: 0.1023s\n",
      "Epoch: 0036 loss_train: 0.6686 acc_train: 0.8667 acc_val: 0.4560 acc_test: 0.4040 time: 0.1024s\n",
      "Epoch: 0037 loss_train: 0.7053 acc_train: 0.8417 acc_val: 0.4420 acc_test: 0.4050 time: 0.1023s\n",
      "Epoch: 0038 loss_train: 0.6725 acc_train: 0.8500 acc_val: 0.4440 acc_test: 0.4050 time: 0.1024s\n",
      "Epoch: 0039 loss_train: 0.6677 acc_train: 0.8417 acc_val: 0.4480 acc_test: 0.3980 time: 0.1022s\n",
      "Epoch: 0040 loss_train: 0.8595 acc_train: 0.7583 acc_val: 0.4420 acc_test: 0.4000 time: 0.1024s\n",
      "Epoch: 0041 loss_train: 0.6648 acc_train: 0.8750 acc_val: 0.4440 acc_test: 0.4050 time: 0.1023s\n",
      "Epoch: 0042 loss_train: 0.6311 acc_train: 0.8667 acc_val: 0.4560 acc_test: 0.4230 time: 0.1022s\n",
      "Epoch: 0043 loss_train: 0.5586 acc_train: 0.9333 acc_val: 0.4560 acc_test: 0.4470 time: 0.1022s\n",
      "Epoch: 0044 loss_train: 0.6819 acc_train: 0.8333 acc_val: 0.4600 acc_test: 0.4390 time: 0.1023s\n",
      "Epoch: 0045 loss_train: 0.7106 acc_train: 0.8583 acc_val: 0.4620 acc_test: 0.4430 time: 0.1022s\n",
      "Epoch: 0046 loss_train: 0.7318 acc_train: 0.8083 acc_val: 0.4780 acc_test: 0.4610 time: 0.1024s\n",
      "Epoch: 0047 loss_train: 0.6201 acc_train: 0.8833 acc_val: 0.4980 acc_test: 0.4920 time: 0.1022s\n",
      "Epoch: 0048 loss_train: 0.6872 acc_train: 0.9333 acc_val: 0.4740 acc_test: 0.4770 time: 0.1023s\n",
      "Epoch: 0049 loss_train: 0.5239 acc_train: 0.9333 acc_val: 0.4660 acc_test: 0.4660 time: 0.1021s\n",
      "Epoch: 0050 loss_train: 0.6412 acc_train: 0.8750 acc_val: 0.4640 acc_test: 0.4660 time: 0.1024s\n",
      "Epoch: 0051 loss_train: 0.6191 acc_train: 0.9083 acc_val: 0.4600 acc_test: 0.4660 time: 0.1022s\n",
      "Epoch: 0052 loss_train: 0.5867 acc_train: 0.8917 acc_val: 0.4660 acc_test: 0.4710 time: 0.1023s\n",
      "Epoch: 0053 loss_train: 0.6652 acc_train: 0.8667 acc_val: 0.4660 acc_test: 0.4740 time: 0.1022s\n",
      "Epoch: 0054 loss_train: 0.5161 acc_train: 0.9333 acc_val: 0.4780 acc_test: 0.4850 time: 0.1023s\n",
      "Epoch: 0055 loss_train: 0.6594 acc_train: 0.9083 acc_val: 0.4920 acc_test: 0.4960 time: 0.1022s\n",
      "Epoch: 0056 loss_train: 0.6138 acc_train: 0.9333 acc_val: 0.5060 acc_test: 0.5100 time: 0.1023s\n",
      "Epoch: 0057 loss_train: 0.6789 acc_train: 0.8417 acc_val: 0.4380 acc_test: 0.4420 time: 0.1023s\n",
      "Epoch: 0058 loss_train: 0.5292 acc_train: 0.9250 acc_val: 0.4300 acc_test: 0.4290 time: 0.1024s\n",
      "Epoch: 0059 loss_train: 0.5934 acc_train: 0.8833 acc_val: 0.4240 acc_test: 0.4220 time: 0.1022s\n",
      "Epoch: 0060 loss_train: 0.5728 acc_train: 0.9333 acc_val: 0.4260 acc_test: 0.4190 time: 0.1024s\n",
      "Epoch: 0061 loss_train: 0.6850 acc_train: 0.8583 acc_val: 0.4380 acc_test: 0.4320 time: 0.1022s\n",
      "Epoch: 0062 loss_train: 0.6069 acc_train: 0.9000 acc_val: 0.4520 acc_test: 0.4380 time: 0.1024s\n",
      "Epoch: 0063 loss_train: 0.7534 acc_train: 0.8417 acc_val: 0.5040 acc_test: 0.4950 time: 0.1023s\n",
      "Epoch: 0064 loss_train: 0.6356 acc_train: 0.9000 acc_val: 0.5020 acc_test: 0.4900 time: 0.1023s\n",
      "Epoch: 0065 loss_train: 0.6421 acc_train: 0.9000 acc_val: 0.4520 acc_test: 0.4420 time: 0.1022s\n",
      "Epoch: 0066 loss_train: 0.6834 acc_train: 0.8833 acc_val: 0.4360 acc_test: 0.4210 time: 0.1022s\n",
      "Epoch: 0067 loss_train: 0.7015 acc_train: 0.8417 acc_val: 0.4420 acc_test: 0.4220 time: 0.1023s\n",
      "Epoch: 0068 loss_train: 0.6117 acc_train: 0.9167 acc_val: 0.4360 acc_test: 0.4220 time: 0.1024s\n",
      "Epoch: 0069 loss_train: 0.6678 acc_train: 0.8167 acc_val: 0.4440 acc_test: 0.4380 time: 0.1021s\n",
      "Epoch: 0070 loss_train: 0.6180 acc_train: 0.9000 acc_val: 0.4440 acc_test: 0.4290 time: 0.1023s\n",
      "Epoch: 0071 loss_train: 0.6685 acc_train: 0.8917 acc_val: 0.4040 acc_test: 0.4040 time: 0.1022s\n",
      "Epoch: 0072 loss_train: 0.6808 acc_train: 0.9000 acc_val: 0.3860 acc_test: 0.3940 time: 0.1023s\n",
      "Epoch: 0073 loss_train: 0.6990 acc_train: 0.8417 acc_val: 0.3980 acc_test: 0.4010 time: 0.1021s\n",
      "Epoch: 0074 loss_train: 0.6341 acc_train: 0.9167 acc_val: 0.4320 acc_test: 0.3970 time: 0.1024s\n",
      "Epoch: 0075 loss_train: 0.6143 acc_train: 0.8750 acc_val: 0.4400 acc_test: 0.4050 time: 0.1023s\n",
      "Epoch: 0076 loss_train: 0.6660 acc_train: 0.8583 acc_val: 0.4500 acc_test: 0.4150 time: 0.1024s\n",
      "Epoch: 0077 loss_train: 0.6356 acc_train: 0.8417 acc_val: 0.5060 acc_test: 0.5070 time: 0.1021s\n",
      "Epoch: 0078 loss_train: 0.5823 acc_train: 0.9000 acc_val: 0.5000 acc_test: 0.5040 time: 0.1023s\n",
      "Epoch: 0079 loss_train: 0.5520 acc_train: 0.9417 acc_val: 0.4960 acc_test: 0.4980 time: 0.1022s\n",
      "Epoch: 0080 loss_train: 0.5671 acc_train: 0.9333 acc_val: 0.4940 acc_test: 0.5060 time: 0.1023s\n",
      "Epoch: 0081 loss_train: 0.5384 acc_train: 0.9167 acc_val: 0.5000 acc_test: 0.5050 time: 0.1022s\n",
      "Epoch: 0082 loss_train: 0.7034 acc_train: 0.8667 acc_val: 0.5060 acc_test: 0.5140 time: 0.1023s\n",
      "Epoch: 0083 loss_train: 0.6288 acc_train: 0.8667 acc_val: 0.4800 acc_test: 0.4700 time: 0.1022s\n",
      "Epoch: 0084 loss_train: 0.5719 acc_train: 0.9083 acc_val: 0.4700 acc_test: 0.4620 time: 0.1024s\n",
      "Epoch: 0085 loss_train: 0.6001 acc_train: 0.9167 acc_val: 0.4680 acc_test: 0.4590 time: 0.1022s\n",
      "Epoch: 0086 loss_train: 0.5804 acc_train: 0.9167 acc_val: 0.4280 acc_test: 0.4000 time: 0.1023s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.6155 acc_train: 0.8500 acc_val: 0.4220 acc_test: 0.3920 time: 0.1023s\n",
      "Epoch: 0088 loss_train: 0.6997 acc_train: 0.8250 acc_val: 0.4180 acc_test: 0.3890 time: 0.1024s\n",
      "Epoch: 0089 loss_train: 0.6092 acc_train: 0.8750 acc_val: 0.4240 acc_test: 0.3930 time: 0.1023s\n",
      "Epoch: 0090 loss_train: 0.6630 acc_train: 0.8750 acc_val: 0.4320 acc_test: 0.3990 time: 0.1023s\n",
      "Epoch: 0091 loss_train: 0.5867 acc_train: 0.8667 acc_val: 0.4500 acc_test: 0.4070 time: 0.1023s\n",
      "Epoch: 0092 loss_train: 0.5774 acc_train: 0.8833 acc_val: 0.4660 acc_test: 0.4140 time: 0.1024s\n",
      "Epoch: 0093 loss_train: 0.6552 acc_train: 0.8750 acc_val: 0.4340 acc_test: 0.4350 time: 0.1021s\n",
      "Epoch: 0094 loss_train: 0.5933 acc_train: 0.8917 acc_val: 0.4460 acc_test: 0.4360 time: 0.1024s\n",
      "Epoch: 0095 loss_train: 0.6021 acc_train: 0.8667 acc_val: 0.4640 acc_test: 0.4520 time: 0.1023s\n",
      "Epoch: 0096 loss_train: 0.6163 acc_train: 0.8750 acc_val: 0.4580 acc_test: 0.4520 time: 0.1023s\n",
      "Epoch: 0097 loss_train: 0.6802 acc_train: 0.8583 acc_val: 0.5080 acc_test: 0.4990 time: 0.1022s\n",
      "Epoch: 0098 loss_train: 0.5834 acc_train: 0.9000 acc_val: 0.4920 acc_test: 0.4860 time: 0.1024s\n",
      "Epoch: 0099 loss_train: 0.6641 acc_train: 0.9083 acc_val: 0.4820 acc_test: 0.4880 time: 0.1021s\n",
      "Epoch: 0100 loss_train: 0.6363 acc_train: 0.9083 acc_val: 0.4780 acc_test: 0.4800 time: 0.1023s\n",
      "Epoch: 0101 loss_train: 0.6444 acc_train: 0.9083 acc_val: 0.4740 acc_test: 0.4860 time: 0.1022s\n",
      "Epoch: 0102 loss_train: 0.5563 acc_train: 0.9167 acc_val: 0.4760 acc_test: 0.4870 time: 0.1023s\n",
      "Epoch: 0103 loss_train: 0.6436 acc_train: 0.8917 acc_val: 0.4840 acc_test: 0.4940 time: 0.1023s\n",
      "Epoch: 0104 loss_train: 0.7112 acc_train: 0.8500 acc_val: 0.4960 acc_test: 0.4980 time: 0.1023s\n",
      "Epoch: 0105 loss_train: 0.5354 acc_train: 0.9167 acc_val: 0.4320 acc_test: 0.3970 time: 0.1022s\n",
      "Epoch: 0106 loss_train: 0.6201 acc_train: 0.9000 acc_val: 0.4280 acc_test: 0.3920 time: 0.1023s\n",
      "Epoch: 0107 loss_train: 0.5987 acc_train: 0.8583 acc_val: 0.4240 acc_test: 0.3870 time: 0.1022s\n",
      "Epoch: 0108 loss_train: 0.5600 acc_train: 0.8583 acc_val: 0.4180 acc_test: 0.3880 time: 0.1023s\n",
      "Epoch: 0109 loss_train: 0.6319 acc_train: 0.8500 acc_val: 0.4240 acc_test: 0.3910 time: 0.1023s\n",
      "Epoch: 0110 loss_train: 0.5996 acc_train: 0.8833 acc_val: 0.4320 acc_test: 0.4100 time: 0.1023s\n",
      "Epoch: 0111 loss_train: 0.6554 acc_train: 0.8583 acc_val: 0.4260 acc_test: 0.4280 time: 0.1024s\n",
      "Epoch: 0112 loss_train: 0.6036 acc_train: 0.8750 acc_val: 0.4180 acc_test: 0.4220 time: 0.1023s\n",
      "Epoch: 0113 loss_train: 0.6598 acc_train: 0.8417 acc_val: 0.4300 acc_test: 0.4230 time: 0.1022s\n",
      "Epoch: 0114 loss_train: 0.6096 acc_train: 0.8833 acc_val: 0.4400 acc_test: 0.4350 time: 0.1023s\n",
      "Epoch: 0115 loss_train: 0.5780 acc_train: 0.8917 acc_val: 0.4340 acc_test: 0.4320 time: 0.1021s\n",
      "Epoch: 0116 loss_train: 0.6841 acc_train: 0.8750 acc_val: 0.4280 acc_test: 0.4170 time: 0.1023s\n",
      "Epoch: 0117 loss_train: 0.6193 acc_train: 0.9083 acc_val: 0.4320 acc_test: 0.4130 time: 0.1021s\n",
      "Epoch: 0118 loss_train: 0.7093 acc_train: 0.8750 acc_val: 0.4300 acc_test: 0.4160 time: 0.1023s\n",
      "Epoch: 0119 loss_train: 0.5982 acc_train: 0.8750 acc_val: 0.4360 acc_test: 0.4250 time: 0.1022s\n",
      "Epoch: 0120 loss_train: 0.5363 acc_train: 0.8833 acc_val: 0.4460 acc_test: 0.4390 time: 0.1024s\n",
      "Epoch: 0121 loss_train: 0.5537 acc_train: 0.9083 acc_val: 0.4980 acc_test: 0.4810 time: 0.1022s\n",
      "Epoch: 0122 loss_train: 0.5474 acc_train: 0.9167 acc_val: 0.4840 acc_test: 0.4690 time: 0.1022s\n",
      "Epoch: 0123 loss_train: 0.6776 acc_train: 0.8333 acc_val: 0.4140 acc_test: 0.4100 time: 0.1021s\n",
      "Epoch: 0124 loss_train: 0.6355 acc_train: 0.8750 acc_val: 0.4720 acc_test: 0.4690 time: 0.1024s\n",
      "Epoch: 0125 loss_train: 0.6007 acc_train: 0.8583 acc_val: 0.4820 acc_test: 0.4780 time: 0.1023s\n",
      "Epoch: 0126 loss_train: 0.5589 acc_train: 0.9250 acc_val: 0.4820 acc_test: 0.4830 time: 0.1024s\n",
      "Epoch: 0127 loss_train: 0.6482 acc_train: 0.9000 acc_val: 0.4280 acc_test: 0.4060 time: 0.1022s\n",
      "Epoch: 0128 loss_train: 0.6222 acc_train: 0.8417 acc_val: 0.4240 acc_test: 0.3950 time: 0.1023s\n",
      "Epoch: 0129 loss_train: 0.5650 acc_train: 0.8750 acc_val: 0.4080 acc_test: 0.3790 time: 0.1023s\n",
      "Epoch: 0130 loss_train: 0.5202 acc_train: 0.8833 acc_val: 0.4020 acc_test: 0.3770 time: 0.1024s\n",
      "Epoch: 0131 loss_train: 0.5584 acc_train: 0.9083 acc_val: 0.4040 acc_test: 0.3750 time: 0.1023s\n",
      "Epoch: 0132 loss_train: 0.7038 acc_train: 0.8167 acc_val: 0.4080 acc_test: 0.3740 time: 0.1024s\n",
      "Epoch: 0133 loss_train: 0.5487 acc_train: 0.8583 acc_val: 0.4060 acc_test: 0.3800 time: 0.1023s\n",
      "Epoch: 0134 loss_train: 0.6834 acc_train: 0.8083 acc_val: 0.4160 acc_test: 0.3870 time: 0.1023s\n",
      "Epoch: 0135 loss_train: 0.6181 acc_train: 0.8750 acc_val: 0.4260 acc_test: 0.3960 time: 0.1023s\n",
      "Epoch: 0136 loss_train: 0.6840 acc_train: 0.8167 acc_val: 0.4600 acc_test: 0.4560 time: 0.1023s\n",
      "Epoch: 0137 loss_train: 0.6556 acc_train: 0.8833 acc_val: 0.4620 acc_test: 0.4580 time: 0.1022s\n",
      "Epoch: 0138 loss_train: 0.5908 acc_train: 0.8667 acc_val: 0.4720 acc_test: 0.4660 time: 0.1023s\n",
      "Epoch: 0139 loss_train: 0.5590 acc_train: 0.9083 acc_val: 0.4840 acc_test: 0.4750 time: 0.1021s\n",
      "Epoch: 0140 loss_train: 0.6183 acc_train: 0.8750 acc_val: 0.4400 acc_test: 0.4410 time: 0.1023s\n",
      "Epoch: 0141 loss_train: 0.6367 acc_train: 0.8500 acc_val: 0.5180 acc_test: 0.5000 time: 0.1022s\n",
      "Epoch: 0142 loss_train: 0.5769 acc_train: 0.9083 acc_val: 0.4940 acc_test: 0.4890 time: 0.1024s\n",
      "Epoch: 0143 loss_train: 0.6421 acc_train: 0.8750 acc_val: 0.4900 acc_test: 0.4850 time: 0.1021s\n",
      "Epoch: 0144 loss_train: 0.6084 acc_train: 0.8750 acc_val: 0.4880 acc_test: 0.4870 time: 0.1024s\n",
      "Epoch: 0145 loss_train: 0.6190 acc_train: 0.8667 acc_val: 0.4880 acc_test: 0.4890 time: 0.1022s\n",
      "Epoch: 0146 loss_train: 0.4556 acc_train: 0.9417 acc_val: 0.4960 acc_test: 0.5000 time: 0.1023s\n",
      "Epoch: 0147 loss_train: 0.6460 acc_train: 0.8667 acc_val: 0.5100 acc_test: 0.5070 time: 0.1022s\n",
      "Epoch: 0148 loss_train: 0.6184 acc_train: 0.8750 acc_val: 0.5200 acc_test: 0.5200 time: 0.1022s\n",
      "Epoch: 0149 loss_train: 0.6014 acc_train: 0.9167 acc_val: 0.4480 acc_test: 0.4500 time: 0.1021s\n",
      "Epoch: 0150 loss_train: 0.6667 acc_train: 0.9000 acc_val: 0.4400 acc_test: 0.4370 time: 0.1023s\n",
      "Epoch: 0151 loss_train: 0.5918 acc_train: 0.8500 acc_val: 0.4360 acc_test: 0.4360 time: 0.1022s\n",
      "Epoch: 0152 loss_train: 0.6515 acc_train: 0.8500 acc_val: 0.4120 acc_test: 0.4340 time: 0.1023s\n",
      "Epoch: 0153 loss_train: 0.5736 acc_train: 0.8750 acc_val: 0.4140 acc_test: 0.4230 time: 0.1023s\n",
      "Epoch: 0154 loss_train: 0.6761 acc_train: 0.8417 acc_val: 0.4160 acc_test: 0.4360 time: 0.1024s\n",
      "Epoch: 0155 loss_train: 0.6150 acc_train: 0.8667 acc_val: 0.4300 acc_test: 0.4460 time: 0.1022s\n",
      "Epoch: 0156 loss_train: 0.4820 acc_train: 0.9083 acc_val: 0.4120 acc_test: 0.4410 time: 0.1023s\n",
      "Epoch: 0157 loss_train: 0.6449 acc_train: 0.8833 acc_val: 0.3960 acc_test: 0.4320 time: 0.1022s\n",
      "Epoch: 0158 loss_train: 0.4851 acc_train: 0.9250 acc_val: 0.3940 acc_test: 0.4240 time: 0.1023s\n",
      "Epoch: 0159 loss_train: 0.5802 acc_train: 0.8500 acc_val: 0.3920 acc_test: 0.4240 time: 0.1023s\n",
      "Epoch: 0160 loss_train: 0.7259 acc_train: 0.8167 acc_val: 0.4000 acc_test: 0.4240 time: 0.1024s\n",
      "Epoch: 0161 loss_train: 0.5839 acc_train: 0.8750 acc_val: 0.4200 acc_test: 0.4220 time: 0.1023s\n",
      "Epoch: 0162 loss_train: 0.4246 acc_train: 0.9167 acc_val: 0.4360 acc_test: 0.4410 time: 0.1024s\n",
      "Epoch: 0163 loss_train: 0.6180 acc_train: 0.8417 acc_val: 0.4160 acc_test: 0.3920 time: 0.1023s\n",
      "Epoch: 0164 loss_train: 0.5937 acc_train: 0.8667 acc_val: 0.4240 acc_test: 0.4020 time: 0.1025s\n",
      "Epoch: 0165 loss_train: 0.5270 acc_train: 0.8917 acc_val: 0.4500 acc_test: 0.4080 time: 0.1023s\n",
      "Epoch: 0166 loss_train: 0.6355 acc_train: 0.8167 acc_val: 0.4680 acc_test: 0.4190 time: 0.1023s\n",
      "Epoch: 0167 loss_train: 0.5412 acc_train: 0.9083 acc_val: 0.4740 acc_test: 0.4620 time: 0.1023s\n",
      "Epoch: 0168 loss_train: 0.6262 acc_train: 0.8833 acc_val: 0.4660 acc_test: 0.4500 time: 0.1024s\n",
      "Epoch: 0169 loss_train: 0.6511 acc_train: 0.9000 acc_val: 0.5160 acc_test: 0.5050 time: 0.1023s\n",
      "Epoch: 0170 loss_train: 0.5752 acc_train: 0.9167 acc_val: 0.5100 acc_test: 0.4990 time: 0.1023s\n",
      "Epoch: 0171 loss_train: 0.5933 acc_train: 0.8917 acc_val: 0.5020 acc_test: 0.4980 time: 0.1022s\n",
      "Epoch: 0172 loss_train: 0.6700 acc_train: 0.8833 acc_val: 0.4980 acc_test: 0.4960 time: 0.1024s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0173 loss_train: 0.6111 acc_train: 0.9167 acc_val: 0.5000 acc_test: 0.5010 time: 0.1021s\n",
      "Epoch: 0174 loss_train: 0.5753 acc_train: 0.8833 acc_val: 0.4420 acc_test: 0.4430 time: 0.1023s\n",
      "Epoch: 0175 loss_train: 0.5628 acc_train: 0.9167 acc_val: 0.4280 acc_test: 0.4290 time: 0.1023s\n",
      "Epoch: 0176 loss_train: 0.5725 acc_train: 0.8917 acc_val: 0.4280 acc_test: 0.4320 time: 0.1022s\n",
      "Epoch: 0177 loss_train: 0.6489 acc_train: 0.8333 acc_val: 0.4300 acc_test: 0.4350 time: 0.1021s\n",
      "Epoch: 0178 loss_train: 0.5768 acc_train: 0.8917 acc_val: 0.4320 acc_test: 0.4330 time: 0.1023s\n",
      "Epoch: 0179 loss_train: 0.6441 acc_train: 0.9000 acc_val: 0.4360 acc_test: 0.4340 time: 0.1023s\n",
      "Epoch: 0180 loss_train: 0.6564 acc_train: 0.8750 acc_val: 0.4380 acc_test: 0.4350 time: 0.1023s\n",
      "Epoch: 0181 loss_train: 0.5061 acc_train: 0.9167 acc_val: 0.4380 acc_test: 0.4410 time: 0.1021s\n",
      "Epoch: 0182 loss_train: 0.6255 acc_train: 0.8250 acc_val: 0.4780 acc_test: 0.4730 time: 0.1024s\n",
      "Epoch: 0183 loss_train: 0.5421 acc_train: 0.8917 acc_val: 0.4820 acc_test: 0.4740 time: 0.1022s\n",
      "Epoch: 0184 loss_train: 0.5162 acc_train: 0.9000 acc_val: 0.4300 acc_test: 0.4290 time: 0.1023s\n",
      "Epoch: 0185 loss_train: 0.6295 acc_train: 0.8833 acc_val: 0.4140 acc_test: 0.4030 time: 0.1021s\n",
      "Epoch: 0186 loss_train: 0.5662 acc_train: 0.8750 acc_val: 0.4100 acc_test: 0.3950 time: 0.1024s\n",
      "Epoch: 0187 loss_train: 0.6108 acc_train: 0.8583 acc_val: 0.4040 acc_test: 0.3940 time: 0.1021s\n",
      "Epoch: 0188 loss_train: 0.6083 acc_train: 0.8500 acc_val: 0.4000 acc_test: 0.3950 time: 0.1024s\n",
      "Epoch: 0189 loss_train: 0.5931 acc_train: 0.8833 acc_val: 0.4040 acc_test: 0.3960 time: 0.1023s\n",
      "Epoch: 0190 loss_train: 0.6041 acc_train: 0.8583 acc_val: 0.4140 acc_test: 0.4010 time: 0.1023s\n",
      "Epoch: 0191 loss_train: 0.5722 acc_train: 0.8667 acc_val: 0.4080 acc_test: 0.4110 time: 0.1021s\n",
      "Epoch: 0192 loss_train: 0.7101 acc_train: 0.8417 acc_val: 0.4340 acc_test: 0.4100 time: 0.1024s\n",
      "Epoch: 0193 loss_train: 0.6683 acc_train: 0.8250 acc_val: 0.4260 acc_test: 0.3940 time: 0.1023s\n",
      "Epoch: 0194 loss_train: 0.5207 acc_train: 0.8667 acc_val: 0.4240 acc_test: 0.3910 time: 0.1023s\n",
      "Epoch: 0195 loss_train: 0.6164 acc_train: 0.8333 acc_val: 0.4300 acc_test: 0.4060 time: 0.1022s\n",
      "Epoch: 0196 loss_train: 0.6212 acc_train: 0.8583 acc_val: 0.4620 acc_test: 0.4690 time: 0.1021s\n",
      "Epoch: 0197 loss_train: 0.5837 acc_train: 0.9083 acc_val: 0.4600 acc_test: 0.4630 time: 0.1023s\n",
      "Epoch: 0198 loss_train: 0.6900 acc_train: 0.8083 acc_val: 0.4520 acc_test: 0.4700 time: 0.1024s\n",
      "Epoch: 0199 loss_train: 0.5626 acc_train: 0.8583 acc_val: 0.4880 acc_test: 0.5120 time: 0.1023s\n",
      "Epoch: 0200 loss_train: 0.7427 acc_train: 0.8333 acc_val: 0.4820 acc_test: 0.5030 time: 0.1024s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 20.6154s\n",
      "0.542 0.533\n",
      "0.524 0.529\n",
      "0.52 0.52\n",
      "0.518 0.5\n",
      "0.516 0.505\n",
      "0.514 0.516\n",
      "0.51 0.499\n",
      "0.508 0.499\n",
      "0.506 0.514\n",
      "0.504 0.495\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN(nfeat=adj.shape[1],\n",
    "        nclass=labels.max().item() + 1,\n",
    "        dropout=0.5)\n",
    "model1.cuda()\n",
    "optimizer = optim.Adam(model1.parameters(),\n",
    "                       lr=0.02, weight_decay=5e-4)\n",
    "t_total = time.time()\n",
    "record = {}\n",
    "for epoch in range(200):\n",
    "    train(epoch,model1,record,features,adj1,adj2,adj3,adj4,adj5)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "bit_list = sorted(record.keys())\n",
    "bit_list.reverse()\n",
    "for key in bit_list[:10]:\n",
    "    value = record[key]\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_adj_sparse = sparse.csr_matrix(A_adj)\n",
    "adj1 = normalize_adj(A_adj_sparse + sp.eye(A_adj.shape[0]))\n",
    "adj1 = torch.FloatTensor(adj1.todense())\n",
    "adj1 = adj1.cuda()\n",
    "adj2 = torch.mm(adj1,adj1)\n",
    "adj3 = torch.mm(adj2,adj1)\n",
    "adj4 = torch.mm(adj3,adj1)\n",
    "adj5 = torch.mm(adj4,adj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.7933 acc_train: 0.1833 acc_val: 0.2380 acc_test: 0.2450 time: 0.1105s\n",
      "Epoch: 0002 loss_train: 1.2142 acc_train: 0.7500 acc_val: 0.3440 acc_test: 0.3380 time: 0.1027s\n",
      "Epoch: 0003 loss_train: 0.6576 acc_train: 0.9583 acc_val: 0.3080 acc_test: 0.2580 time: 0.1023s\n",
      "Epoch: 0004 loss_train: 0.3955 acc_train: 0.9583 acc_val: 0.3000 acc_test: 0.2530 time: 0.1023s\n",
      "Epoch: 0005 loss_train: 0.2857 acc_train: 0.9500 acc_val: 0.2900 acc_test: 0.2600 time: 0.1022s\n",
      "Epoch: 0006 loss_train: 0.2032 acc_train: 0.9583 acc_val: 0.3000 acc_test: 0.2610 time: 0.1024s\n",
      "Epoch: 0007 loss_train: 0.1293 acc_train: 0.9917 acc_val: 0.2580 acc_test: 0.2810 time: 0.1023s\n",
      "Epoch: 0008 loss_train: 0.1470 acc_train: 0.9833 acc_val: 0.2680 acc_test: 0.2640 time: 0.1024s\n",
      "Epoch: 0009 loss_train: 0.1159 acc_train: 0.9917 acc_val: 0.2780 acc_test: 0.2580 time: 0.1022s\n",
      "Epoch: 0010 loss_train: 0.1104 acc_train: 0.9750 acc_val: 0.2780 acc_test: 0.2470 time: 0.1023s\n",
      "Epoch: 0011 loss_train: 0.1625 acc_train: 0.9667 acc_val: 0.2820 acc_test: 0.2430 time: 0.1023s\n",
      "Epoch: 0012 loss_train: 0.1343 acc_train: 0.9750 acc_val: 0.2880 acc_test: 0.2390 time: 0.1024s\n",
      "Epoch: 0013 loss_train: 0.1290 acc_train: 0.9750 acc_val: 0.2840 acc_test: 0.2370 time: 0.1023s\n",
      "Epoch: 0014 loss_train: 0.0928 acc_train: 1.0000 acc_val: 0.2900 acc_test: 0.2350 time: 0.1024s\n",
      "Epoch: 0015 loss_train: 0.1310 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2300 time: 0.1023s\n",
      "Epoch: 0016 loss_train: 0.1280 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2330 time: 0.1022s\n",
      "Epoch: 0017 loss_train: 0.1356 acc_train: 0.9833 acc_val: 0.2800 acc_test: 0.2340 time: 0.1023s\n",
      "Epoch: 0018 loss_train: 0.1340 acc_train: 0.9833 acc_val: 0.2780 acc_test: 0.2340 time: 0.1024s\n",
      "Epoch: 0019 loss_train: 0.1548 acc_train: 0.9750 acc_val: 0.2820 acc_test: 0.2360 time: 0.1022s\n",
      "Epoch: 0020 loss_train: 0.1107 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2390 time: 0.1024s\n",
      "Epoch: 0021 loss_train: 0.1292 acc_train: 0.9917 acc_val: 0.2880 acc_test: 0.2420 time: 0.1021s\n",
      "Epoch: 0022 loss_train: 0.1290 acc_train: 0.9917 acc_val: 0.2900 acc_test: 0.2430 time: 0.1023s\n",
      "Epoch: 0023 loss_train: 0.1404 acc_train: 0.9833 acc_val: 0.3260 acc_test: 0.2650 time: 0.1022s\n",
      "Epoch: 0024 loss_train: 0.1491 acc_train: 0.9917 acc_val: 0.3040 acc_test: 0.2850 time: 0.1021s\n",
      "Epoch: 0025 loss_train: 0.1525 acc_train: 0.9917 acc_val: 0.3040 acc_test: 0.2790 time: 0.1021s\n",
      "Epoch: 0026 loss_train: 0.1646 acc_train: 0.9750 acc_val: 0.2800 acc_test: 0.2810 time: 0.1023s\n",
      "Epoch: 0027 loss_train: 0.1401 acc_train: 0.9917 acc_val: 0.2940 acc_test: 0.2960 time: 0.1023s\n",
      "Epoch: 0028 loss_train: 0.1618 acc_train: 0.9833 acc_val: 0.3100 acc_test: 0.2930 time: 0.1023s\n",
      "Epoch: 0029 loss_train: 0.1496 acc_train: 0.9917 acc_val: 0.3000 acc_test: 0.2920 time: 0.1021s\n",
      "Epoch: 0030 loss_train: 0.1434 acc_train: 1.0000 acc_val: 0.2920 acc_test: 0.2900 time: 0.1024s\n",
      "Epoch: 0031 loss_train: 0.1501 acc_train: 0.9917 acc_val: 0.2800 acc_test: 0.2820 time: 0.1023s\n",
      "Epoch: 0032 loss_train: 0.1866 acc_train: 0.9750 acc_val: 0.2860 acc_test: 0.2460 time: 0.1023s\n",
      "Epoch: 0033 loss_train: 0.1395 acc_train: 1.0000 acc_val: 0.2880 acc_test: 0.2410 time: 0.1023s\n",
      "Epoch: 0034 loss_train: 0.1548 acc_train: 0.9750 acc_val: 0.2880 acc_test: 0.2390 time: 0.1024s\n",
      "Epoch: 0035 loss_train: 0.1450 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2390 time: 0.1022s\n",
      "Epoch: 0036 loss_train: 0.1465 acc_train: 1.0000 acc_val: 0.2840 acc_test: 0.2400 time: 0.1022s\n",
      "Epoch: 0037 loss_train: 0.1754 acc_train: 0.9833 acc_val: 0.3080 acc_test: 0.2620 time: 0.1021s\n",
      "Epoch: 0038 loss_train: 0.1709 acc_train: 0.9750 acc_val: 0.2820 acc_test: 0.2400 time: 0.1023s\n",
      "Epoch: 0039 loss_train: 0.1497 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2420 time: 0.1023s\n",
      "Epoch: 0040 loss_train: 0.1457 acc_train: 0.9750 acc_val: 0.2900 acc_test: 0.2420 time: 0.1023s\n",
      "Epoch: 0041 loss_train: 0.2453 acc_train: 0.9417 acc_val: 0.2900 acc_test: 0.2420 time: 0.1023s\n",
      "Epoch: 0042 loss_train: 0.1593 acc_train: 1.0000 acc_val: 0.2880 acc_test: 0.2420 time: 0.1025s\n",
      "Epoch: 0043 loss_train: 0.1381 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2410 time: 0.1023s\n",
      "Epoch: 0044 loss_train: 0.1461 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2410 time: 0.1023s\n",
      "Epoch: 0045 loss_train: 0.1312 acc_train: 1.0000 acc_val: 0.2840 acc_test: 0.2390 time: 0.1022s\n",
      "Epoch: 0046 loss_train: 0.1615 acc_train: 0.9833 acc_val: 0.2820 acc_test: 0.2410 time: 0.1023s\n",
      "Epoch: 0047 loss_train: 0.1388 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2380 time: 0.1021s\n",
      "Epoch: 0048 loss_train: 0.1562 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2370 time: 0.1024s\n",
      "Epoch: 0049 loss_train: 0.1976 acc_train: 0.9833 acc_val: 0.2820 acc_test: 0.2350 time: 0.1022s\n",
      "Epoch: 0050 loss_train: 0.1215 acc_train: 1.0000 acc_val: 0.2780 acc_test: 0.2330 time: 0.1024s\n",
      "Epoch: 0051 loss_train: 0.1538 acc_train: 0.9833 acc_val: 0.2760 acc_test: 0.2340 time: 0.1021s\n",
      "Epoch: 0052 loss_train: 0.1500 acc_train: 0.9917 acc_val: 0.2780 acc_test: 0.2350 time: 0.1023s\n",
      "Epoch: 0053 loss_train: 0.2067 acc_train: 0.9667 acc_val: 0.2800 acc_test: 0.2350 time: 0.1022s\n",
      "Epoch: 0054 loss_train: 0.1454 acc_train: 0.9750 acc_val: 0.2800 acc_test: 0.2360 time: 0.1024s\n",
      "Epoch: 0055 loss_train: 0.1470 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2400 time: 0.1022s\n",
      "Epoch: 0056 loss_train: 0.1767 acc_train: 0.9750 acc_val: 0.2860 acc_test: 0.2410 time: 0.1023s\n",
      "Epoch: 0057 loss_train: 0.1599 acc_train: 0.9917 acc_val: 0.3020 acc_test: 0.2510 time: 0.1021s\n",
      "Epoch: 0058 loss_train: 0.1370 acc_train: 1.0000 acc_val: 0.2920 acc_test: 0.2990 time: 0.1024s\n",
      "Epoch: 0059 loss_train: 0.1318 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2910 time: 0.1022s\n",
      "Epoch: 0060 loss_train: 0.1386 acc_train: 0.9917 acc_val: 0.2780 acc_test: 0.2330 time: 0.1023s\n",
      "Epoch: 0061 loss_train: 0.1706 acc_train: 0.9917 acc_val: 0.2740 acc_test: 0.2340 time: 0.1021s\n",
      "Epoch: 0062 loss_train: 0.1578 acc_train: 0.9917 acc_val: 0.2760 acc_test: 0.2330 time: 0.1024s\n",
      "Epoch: 0063 loss_train: 0.1698 acc_train: 0.9750 acc_val: 0.2760 acc_test: 0.2320 time: 0.1022s\n",
      "Epoch: 0064 loss_train: 0.1874 acc_train: 0.9750 acc_val: 0.2760 acc_test: 0.2330 time: 0.1023s\n",
      "Epoch: 0065 loss_train: 0.1680 acc_train: 0.9833 acc_val: 0.2720 acc_test: 0.2330 time: 0.1022s\n",
      "Epoch: 0066 loss_train: 0.1736 acc_train: 0.9917 acc_val: 0.2780 acc_test: 0.2370 time: 0.1023s\n",
      "Epoch: 0067 loss_train: 0.1789 acc_train: 0.9750 acc_val: 0.2820 acc_test: 0.2920 time: 0.1022s\n",
      "Epoch: 0068 loss_train: 0.1715 acc_train: 0.9833 acc_val: 0.2860 acc_test: 0.2930 time: 0.1023s\n",
      "Epoch: 0069 loss_train: 0.1468 acc_train: 0.9750 acc_val: 0.2940 acc_test: 0.2970 time: 0.1021s\n",
      "Epoch: 0070 loss_train: 0.1832 acc_train: 0.9750 acc_val: 0.2900 acc_test: 0.2400 time: 0.1023s\n",
      "Epoch: 0071 loss_train: 0.1666 acc_train: 0.9917 acc_val: 0.2920 acc_test: 0.2380 time: 0.1022s\n",
      "Epoch: 0072 loss_train: 0.1159 acc_train: 1.0000 acc_val: 0.2880 acc_test: 0.2340 time: 0.1023s\n",
      "Epoch: 0073 loss_train: 0.1439 acc_train: 0.9833 acc_val: 0.2820 acc_test: 0.2350 time: 0.1023s\n",
      "Epoch: 0074 loss_train: 0.1803 acc_train: 0.9833 acc_val: 0.2860 acc_test: 0.2370 time: 0.1024s\n",
      "Epoch: 0075 loss_train: 0.1409 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2390 time: 0.1023s\n",
      "Epoch: 0076 loss_train: 0.1497 acc_train: 1.0000 acc_val: 0.2860 acc_test: 0.2400 time: 0.1023s\n",
      "Epoch: 0077 loss_train: 0.1593 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2400 time: 0.1023s\n",
      "Epoch: 0078 loss_train: 0.1458 acc_train: 1.0000 acc_val: 0.2840 acc_test: 0.2840 time: 0.1024s\n",
      "Epoch: 0079 loss_train: 0.1518 acc_train: 0.9833 acc_val: 0.2900 acc_test: 0.2930 time: 0.1024s\n",
      "Epoch: 0080 loss_train: 0.1166 acc_train: 1.0000 acc_val: 0.2940 acc_test: 0.2940 time: 0.1024s\n",
      "Epoch: 0081 loss_train: 0.1413 acc_train: 0.9917 acc_val: 0.2900 acc_test: 0.2420 time: 0.1023s\n",
      "Epoch: 0082 loss_train: 0.1535 acc_train: 0.9917 acc_val: 0.2960 acc_test: 0.2400 time: 0.1022s\n",
      "Epoch: 0083 loss_train: 0.1396 acc_train: 1.0000 acc_val: 0.2880 acc_test: 0.2410 time: 0.1022s\n",
      "Epoch: 0084 loss_train: 0.1462 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2400 time: 0.1022s\n",
      "Epoch: 0085 loss_train: 0.1393 acc_train: 1.0000 acc_val: 0.2940 acc_test: 0.2400 time: 0.1022s\n",
      "Epoch: 0086 loss_train: 0.1281 acc_train: 1.0000 acc_val: 0.2960 acc_test: 0.2410 time: 0.1021s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.1536 acc_train: 0.9833 acc_val: 0.3080 acc_test: 0.2490 time: 0.1023s\n",
      "Epoch: 0088 loss_train: 0.1400 acc_train: 1.0000 acc_val: 0.3100 acc_test: 0.2710 time: 0.1022s\n",
      "Epoch: 0089 loss_train: 0.1509 acc_train: 0.9833 acc_val: 0.3040 acc_test: 0.2720 time: 0.1022s\n",
      "Epoch: 0090 loss_train: 0.1744 acc_train: 0.9750 acc_val: 0.3220 acc_test: 0.3120 time: 0.1024s\n",
      "Epoch: 0091 loss_train: 0.1788 acc_train: 0.9833 acc_val: 0.3080 acc_test: 0.3000 time: 0.1022s\n",
      "Epoch: 0092 loss_train: 0.1646 acc_train: 0.9750 acc_val: 0.3000 acc_test: 0.2980 time: 0.1024s\n",
      "Epoch: 0093 loss_train: 0.1405 acc_train: 1.0000 acc_val: 0.3080 acc_test: 0.2970 time: 0.1024s\n",
      "Epoch: 0094 loss_train: 0.1700 acc_train: 0.9750 acc_val: 0.3240 acc_test: 0.3130 time: 0.1023s\n",
      "Epoch: 0095 loss_train: 0.1187 acc_train: 1.0000 acc_val: 0.3240 acc_test: 0.2660 time: 0.1022s\n",
      "Epoch: 0096 loss_train: 0.1392 acc_train: 0.9833 acc_val: 0.2960 acc_test: 0.2430 time: 0.1023s\n",
      "Epoch: 0097 loss_train: 0.1416 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2390 time: 0.1022s\n",
      "Epoch: 0098 loss_train: 0.1668 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2360 time: 0.1023s\n",
      "Epoch: 0099 loss_train: 0.1661 acc_train: 0.9833 acc_val: 0.2840 acc_test: 0.2380 time: 0.1023s\n",
      "Epoch: 0100 loss_train: 0.1699 acc_train: 0.9667 acc_val: 0.2960 acc_test: 0.2410 time: 0.1023s\n",
      "Epoch: 0101 loss_train: 0.1618 acc_train: 0.9833 acc_val: 0.3100 acc_test: 0.2570 time: 0.1023s\n",
      "Epoch: 0102 loss_train: 0.1640 acc_train: 0.9917 acc_val: 0.2980 acc_test: 0.2710 time: 0.1023s\n",
      "Epoch: 0103 loss_train: 0.1368 acc_train: 0.9917 acc_val: 0.3000 acc_test: 0.2720 time: 0.1022s\n",
      "Epoch: 0104 loss_train: 0.1452 acc_train: 1.0000 acc_val: 0.3080 acc_test: 0.2730 time: 0.1024s\n",
      "Epoch: 0105 loss_train: 0.2167 acc_train: 0.9500 acc_val: 0.3060 acc_test: 0.2950 time: 0.1022s\n",
      "Epoch: 0106 loss_train: 0.1901 acc_train: 0.9750 acc_val: 0.2920 acc_test: 0.2410 time: 0.1024s\n",
      "Epoch: 0107 loss_train: 0.1294 acc_train: 0.9917 acc_val: 0.3020 acc_test: 0.2450 time: 0.1023s\n",
      "Epoch: 0108 loss_train: 0.1785 acc_train: 0.9750 acc_val: 0.3060 acc_test: 0.2480 time: 0.1023s\n",
      "Epoch: 0109 loss_train: 0.1831 acc_train: 0.9833 acc_val: 0.2980 acc_test: 0.2410 time: 0.1022s\n",
      "Epoch: 0110 loss_train: 0.1756 acc_train: 0.9750 acc_val: 0.3100 acc_test: 0.2540 time: 0.1023s\n",
      "Epoch: 0111 loss_train: 0.1595 acc_train: 0.9917 acc_val: 0.3120 acc_test: 0.2600 time: 0.1023s\n",
      "Epoch: 0112 loss_train: 0.1364 acc_train: 0.9917 acc_val: 0.2960 acc_test: 0.2970 time: 0.1024s\n",
      "Epoch: 0113 loss_train: 0.1465 acc_train: 0.9750 acc_val: 0.2820 acc_test: 0.2910 time: 0.1023s\n",
      "Epoch: 0114 loss_train: 0.1686 acc_train: 0.9667 acc_val: 0.2920 acc_test: 0.2950 time: 0.1024s\n",
      "Epoch: 0115 loss_train: 0.1388 acc_train: 0.9917 acc_val: 0.2800 acc_test: 0.2370 time: 0.1022s\n",
      "Epoch: 0116 loss_train: 0.1332 acc_train: 1.0000 acc_val: 0.2780 acc_test: 0.2400 time: 0.1022s\n",
      "Epoch: 0117 loss_train: 0.2092 acc_train: 0.9667 acc_val: 0.2840 acc_test: 0.2360 time: 0.1021s\n",
      "Epoch: 0118 loss_train: 0.1399 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2370 time: 0.1023s\n",
      "Epoch: 0119 loss_train: 0.1564 acc_train: 0.9917 acc_val: 0.2920 acc_test: 0.2950 time: 0.1022s\n",
      "Epoch: 0120 loss_train: 0.1343 acc_train: 1.0000 acc_val: 0.2840 acc_test: 0.2880 time: 0.1023s\n",
      "Epoch: 0121 loss_train: 0.1718 acc_train: 0.9667 acc_val: 0.2860 acc_test: 0.2900 time: 0.1022s\n",
      "Epoch: 0122 loss_train: 0.1820 acc_train: 0.9833 acc_val: 0.2800 acc_test: 0.2860 time: 0.1022s\n",
      "Epoch: 0123 loss_train: 0.1673 acc_train: 0.9667 acc_val: 0.2880 acc_test: 0.2900 time: 0.1021s\n",
      "Epoch: 0124 loss_train: 0.1591 acc_train: 0.9833 acc_val: 0.2940 acc_test: 0.3020 time: 0.1024s\n",
      "Epoch: 0125 loss_train: 0.1585 acc_train: 0.9833 acc_val: 0.3020 acc_test: 0.2550 time: 0.1022s\n",
      "Epoch: 0126 loss_train: 0.1565 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2420 time: 0.1023s\n",
      "Epoch: 0127 loss_train: 0.1799 acc_train: 0.9750 acc_val: 0.2920 acc_test: 0.2380 time: 0.1022s\n",
      "Epoch: 0128 loss_train: 0.1409 acc_train: 1.0000 acc_val: 0.2900 acc_test: 0.2400 time: 0.1023s\n",
      "Epoch: 0129 loss_train: 0.1335 acc_train: 0.9917 acc_val: 0.2920 acc_test: 0.2380 time: 0.1023s\n",
      "Epoch: 0130 loss_train: 0.1444 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2890 time: 0.1022s\n",
      "Epoch: 0131 loss_train: 0.1693 acc_train: 0.9833 acc_val: 0.2780 acc_test: 0.2820 time: 0.1022s\n",
      "Epoch: 0132 loss_train: 0.1708 acc_train: 0.9750 acc_val: 0.2660 acc_test: 0.2800 time: 0.1023s\n",
      "Epoch: 0133 loss_train: 0.1652 acc_train: 0.9667 acc_val: 0.2840 acc_test: 0.2910 time: 0.1022s\n",
      "Epoch: 0134 loss_train: 0.1484 acc_train: 0.9833 acc_val: 0.2980 acc_test: 0.2970 time: 0.1023s\n",
      "Epoch: 0135 loss_train: 0.1522 acc_train: 0.9917 acc_val: 0.2880 acc_test: 0.2400 time: 0.1020s\n",
      "Epoch: 0136 loss_train: 0.1610 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2390 time: 0.1023s\n",
      "Epoch: 0137 loss_train: 0.1505 acc_train: 0.9917 acc_val: 0.2880 acc_test: 0.2380 time: 0.1021s\n",
      "Epoch: 0138 loss_train: 0.1366 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2380 time: 0.1022s\n",
      "Epoch: 0139 loss_train: 0.1489 acc_train: 0.9917 acc_val: 0.2940 acc_test: 0.2950 time: 0.1023s\n",
      "Epoch: 0140 loss_train: 0.1303 acc_train: 1.0000 acc_val: 0.2720 acc_test: 0.2820 time: 0.1023s\n",
      "Epoch: 0141 loss_train: 0.1350 acc_train: 1.0000 acc_val: 0.2720 acc_test: 0.2810 time: 0.1023s\n",
      "Epoch: 0142 loss_train: 0.1362 acc_train: 0.9917 acc_val: 0.2740 acc_test: 0.2820 time: 0.1023s\n",
      "Epoch: 0143 loss_train: 0.1638 acc_train: 0.9750 acc_val: 0.2760 acc_test: 0.2590 time: 0.1024s\n",
      "Epoch: 0144 loss_train: 0.1530 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2620 time: 0.1022s\n",
      "Epoch: 0145 loss_train: 0.1647 acc_train: 0.9917 acc_val: 0.2980 acc_test: 0.2670 time: 0.1022s\n",
      "Epoch: 0146 loss_train: 0.1448 acc_train: 0.9917 acc_val: 0.3020 acc_test: 0.2420 time: 0.1024s\n",
      "Epoch: 0147 loss_train: 0.1475 acc_train: 0.9917 acc_val: 0.2880 acc_test: 0.2390 time: 0.1024s\n",
      "Epoch: 0148 loss_train: 0.1320 acc_train: 1.0000 acc_val: 0.2820 acc_test: 0.2340 time: 0.1023s\n",
      "Epoch: 0149 loss_train: 0.1358 acc_train: 1.0000 acc_val: 0.2820 acc_test: 0.2350 time: 0.1022s\n",
      "Epoch: 0150 loss_train: 0.1430 acc_train: 1.0000 acc_val: 0.2840 acc_test: 0.2380 time: 0.1023s\n",
      "Epoch: 0151 loss_train: 0.1430 acc_train: 0.9917 acc_val: 0.2800 acc_test: 0.2380 time: 0.1023s\n",
      "Epoch: 0152 loss_train: 0.1460 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2900 time: 0.1023s\n",
      "Epoch: 0153 loss_train: 0.1406 acc_train: 1.0000 acc_val: 0.2760 acc_test: 0.2840 time: 0.1023s\n",
      "Epoch: 0154 loss_train: 0.1440 acc_train: 0.9917 acc_val: 0.2740 acc_test: 0.2860 time: 0.1024s\n",
      "Epoch: 0155 loss_train: 0.1694 acc_train: 0.9833 acc_val: 0.2960 acc_test: 0.2920 time: 0.1021s\n",
      "Epoch: 0156 loss_train: 0.1553 acc_train: 0.9917 acc_val: 0.3000 acc_test: 0.2940 time: 0.1024s\n",
      "Epoch: 0157 loss_train: 0.1657 acc_train: 0.9833 acc_val: 0.2960 acc_test: 0.2930 time: 0.1022s\n",
      "Epoch: 0158 loss_train: 0.1239 acc_train: 1.0000 acc_val: 0.2900 acc_test: 0.2920 time: 0.1022s\n",
      "Epoch: 0159 loss_train: 0.1685 acc_train: 0.9750 acc_val: 0.2760 acc_test: 0.2340 time: 0.1022s\n",
      "Epoch: 0160 loss_train: 0.2054 acc_train: 0.9750 acc_val: 0.2760 acc_test: 0.2330 time: 0.1024s\n",
      "Epoch: 0161 loss_train: 0.1179 acc_train: 1.0000 acc_val: 0.2820 acc_test: 0.2350 time: 0.1022s\n",
      "Epoch: 0162 loss_train: 0.1537 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2350 time: 0.1023s\n",
      "Epoch: 0163 loss_train: 0.1661 acc_train: 1.0000 acc_val: 0.2840 acc_test: 0.2350 time: 0.1024s\n",
      "Epoch: 0164 loss_train: 0.1232 acc_train: 1.0000 acc_val: 0.2840 acc_test: 0.2370 time: 0.1024s\n",
      "Epoch: 0165 loss_train: 0.1317 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2380 time: 0.1023s\n",
      "Epoch: 0166 loss_train: 0.1464 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2390 time: 0.1023s\n",
      "Epoch: 0167 loss_train: 0.1546 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2910 time: 0.1023s\n",
      "Epoch: 0168 loss_train: 0.1589 acc_train: 0.9917 acc_val: 0.2960 acc_test: 0.2950 time: 0.1024s\n",
      "Epoch: 0169 loss_train: 0.1427 acc_train: 1.0000 acc_val: 0.2860 acc_test: 0.2390 time: 0.1023s\n",
      "Epoch: 0170 loss_train: 0.1450 acc_train: 0.9833 acc_val: 0.2860 acc_test: 0.2390 time: 0.1023s\n",
      "Epoch: 0171 loss_train: 0.1551 acc_train: 0.9833 acc_val: 0.2860 acc_test: 0.2360 time: 0.1022s\n",
      "Epoch: 0172 loss_train: 0.1704 acc_train: 0.9750 acc_val: 0.2860 acc_test: 0.2390 time: 0.1023s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0173 loss_train: 0.1515 acc_train: 0.9917 acc_val: 0.2900 acc_test: 0.2410 time: 0.1022s\n",
      "Epoch: 0174 loss_train: 0.1970 acc_train: 0.9833 acc_val: 0.2860 acc_test: 0.2390 time: 0.1022s\n",
      "Epoch: 0175 loss_train: 0.1587 acc_train: 0.9917 acc_val: 0.2900 acc_test: 0.2400 time: 0.1022s\n",
      "Epoch: 0176 loss_train: 0.1734 acc_train: 0.9750 acc_val: 0.2820 acc_test: 0.2390 time: 0.1023s\n",
      "Epoch: 0177 loss_train: 0.1627 acc_train: 0.9750 acc_val: 0.2940 acc_test: 0.2930 time: 0.1023s\n",
      "Epoch: 0178 loss_train: 0.1592 acc_train: 0.9833 acc_val: 0.3080 acc_test: 0.2910 time: 0.1022s\n",
      "Epoch: 0179 loss_train: 0.1579 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2370 time: 0.1022s\n",
      "Epoch: 0180 loss_train: 0.1268 acc_train: 1.0000 acc_val: 0.2820 acc_test: 0.2380 time: 0.1022s\n",
      "Epoch: 0181 loss_train: 0.1580 acc_train: 0.9917 acc_val: 0.2840 acc_test: 0.2350 time: 0.1032s\n",
      "Epoch: 0182 loss_train: 0.1326 acc_train: 0.9917 acc_val: 0.2820 acc_test: 0.2340 time: 0.1024s\n",
      "Epoch: 0183 loss_train: 0.1521 acc_train: 0.9750 acc_val: 0.2800 acc_test: 0.2340 time: 0.1022s\n",
      "Epoch: 0184 loss_train: 0.1752 acc_train: 0.9833 acc_val: 0.2860 acc_test: 0.2380 time: 0.1025s\n",
      "Epoch: 0185 loss_train: 0.1956 acc_train: 0.9667 acc_val: 0.2860 acc_test: 0.2400 time: 0.1022s\n",
      "Epoch: 0186 loss_train: 0.1560 acc_train: 0.9917 acc_val: 0.2900 acc_test: 0.2970 time: 0.1023s\n",
      "Epoch: 0187 loss_train: 0.1490 acc_train: 0.9750 acc_val: 0.2960 acc_test: 0.2960 time: 0.1023s\n",
      "Epoch: 0188 loss_train: 0.1850 acc_train: 0.9750 acc_val: 0.2980 acc_test: 0.2930 time: 0.1022s\n",
      "Epoch: 0189 loss_train: 0.1459 acc_train: 0.9917 acc_val: 0.2900 acc_test: 0.2400 time: 0.1023s\n",
      "Epoch: 0190 loss_train: 0.1506 acc_train: 0.9917 acc_val: 0.2900 acc_test: 0.2410 time: 0.1024s\n",
      "Epoch: 0191 loss_train: 0.1626 acc_train: 0.9833 acc_val: 0.2960 acc_test: 0.2490 time: 0.1021s\n",
      "Epoch: 0192 loss_train: 0.2119 acc_train: 0.9833 acc_val: 0.2980 acc_test: 0.2440 time: 0.1022s\n",
      "Epoch: 0193 loss_train: 0.1595 acc_train: 1.0000 acc_val: 0.2880 acc_test: 0.2400 time: 0.1023s\n",
      "Epoch: 0194 loss_train: 0.1672 acc_train: 0.9750 acc_val: 0.2840 acc_test: 0.2350 time: 0.1025s\n",
      "Epoch: 0195 loss_train: 0.1322 acc_train: 1.0000 acc_val: 0.2780 acc_test: 0.2330 time: 0.1021s\n",
      "Epoch: 0196 loss_train: 0.1442 acc_train: 0.9917 acc_val: 0.2860 acc_test: 0.2360 time: 0.1023s\n",
      "Epoch: 0197 loss_train: 0.1725 acc_train: 0.9833 acc_val: 0.2860 acc_test: 0.2360 time: 0.1023s\n",
      "Epoch: 0198 loss_train: 0.1676 acc_train: 1.0000 acc_val: 0.2880 acc_test: 0.2360 time: 0.1024s\n",
      "Epoch: 0199 loss_train: 0.1349 acc_train: 0.9833 acc_val: 0.2900 acc_test: 0.2360 time: 0.1023s\n",
      "Epoch: 0200 loss_train: 0.1396 acc_train: 1.0000 acc_val: 0.2820 acc_test: 0.2350 time: 0.1024s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 20.6040s\n",
      "0.34400000000000003 0.338\n",
      "0.326 0.265\n",
      "0.324 0.266\n",
      "0.322 0.312\n",
      "0.312 0.26\n",
      "0.31 0.254\n",
      "0.308 0.291\n",
      "0.306 0.248\n",
      "0.304 0.272\n",
      "0.302 0.242\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN(nfeat=adj.shape[1],\n",
    "        nclass=labels.max().item() + 1,\n",
    "        dropout=0.5)\n",
    "model1.cuda()\n",
    "optimizer = optim.Adam(model1.parameters(),\n",
    "                       lr=0.02, weight_decay=5e-4)\n",
    "t_total = time.time()\n",
    "record = {}\n",
    "for epoch in range(200):\n",
    "    train(epoch,model1,record,features,adj1,adj2,adj3,adj4,adj5)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "bit_list = sorted(record.keys())\n",
    "bit_list.reverse()\n",
    "for key in bit_list[:10]:\n",
    "    value = record[key]\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustering-DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparts = 1000\n",
    "eps = 0.6\n",
    "def DP(adj):  \n",
    "    g=nx.Graph(adj)\n",
    "    num_nodes=g.number_of_nodes()\n",
    "    g_list=list()\n",
    "    for i in range(num_nodes):\n",
    "        nodes = np.where(adj[i] > 0)\n",
    "        nodes = nodes[0]\n",
    "        nodes = nodes.tolist()\n",
    "        g_list.append(nodes)\n",
    "    (edgecuts,parts)=pymetis.part_graph(nparts=nparts,adjacency=g_list)\n",
    "    g_dic=dict()\n",
    "    for i in range(nparts):\n",
    "        g_dic[i] = []\n",
    "    for i in range(num_nodes):\n",
    "        index = parts[i]\n",
    "        g_dic[index].append(i)\n",
    "    P_array = np.zeros([adj.shape[0],nparts])\n",
    "    count = 0\n",
    "    for i in range(adj.shape[0]):\n",
    "        for key in g_dic.keys():\n",
    "            if i in g_dic[key]:\n",
    "                P_array[i][key] = len(g_dic[key])\n",
    "        neighbors = g_list[i]\n",
    "        for neighbor in neighbors:\n",
    "            for key in g_dic.keys():\n",
    "                if neighbor in g_dic[key] and neighbor != i:\n",
    "                    P_array[i][key] += 1\n",
    "    P_array_01 = copy.deepcopy(P_array)\n",
    "    P_array_01[P_array_01>0] =1\n",
    "    n = P_array_01.shape[0]\n",
    "    m=P_array_01.shape[1]\n",
    "    count = 0\n",
    "    B_edges = []\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if P_array_01[i][j]>0:\n",
    "                B_edges.append((i,j))\n",
    "                count = count+1\n",
    "    B_adj = P_array_01\n",
    "    B_adj_dp = np.zeros((n,m))\n",
    "    eps_1 =eps*math.log(n)\n",
    "    eps_2 = 1\n",
    "    m_new = int(len(B_edges) + np.random.laplace(0, 1/eps_2, 1))\n",
    "    eps_t = math.log(2*m*n/(2*m_new)-1)\n",
    "    if eps_1 < eps_t:\n",
    "        theta = 1/(2*eps_1)*eps_t\n",
    "    else:\n",
    "        theta = 1/(2*eps_1)*math.log(2*m*n/(4*m_new)+1/2*(math.exp(eps_1)-1))\n",
    "    n1=0\n",
    "    for edge in B_edges:\n",
    "        i = edge[0]\n",
    "        j= edge[1]\n",
    "        B_adj_ij = B_adj[i][j] + np.random.laplace(0, 1/eps_1, 1)\n",
    "        if B_adj_ij > theta:\n",
    "            B_adj_dp[i][j]=1\n",
    "            n1+=1\n",
    "    n0=m_new-n1\n",
    "\n",
    "\n",
    "    while n0 > 0:\n",
    "        i = random.randint(0,n-1)\n",
    "        j = random.randint(0,m-1)\n",
    "        if B_adj[i][j]!=1 and B_adj_dp[i][j] !=1:\n",
    "            B_adj_dp[i][j] =1\n",
    "            n0 = n0-1   \n",
    "    noise = B_adj - B_adj_dp\n",
    "    P_ = P_array - noise\n",
    "    Q_array = np.zeros([nparts,adj.shape[0]])\n",
    "    for key in g_dic.keys():\n",
    "        cluster_neighbors = g_dic[key]\n",
    "        cluster_len = len(cluster_neighbors)\n",
    "        for neighbor in cluster_neighbors:\n",
    "            Q_array[key][neighbor] = 1/cluster_len\n",
    "    A_ = np.matmul(P_, Q_array)\n",
    "    return A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_adj_dp = DP(B_adj+np.eye(B_adj.shape[0]))\n",
    "A_adj_new = A_adj + B_adj_dp\n",
    "A_adj_sparse = sparse.csr_matrix(A_adj_new)\n",
    "adj1 = normalize_adj(A_adj_sparse  )\n",
    "adj1 = torch.FloatTensor(adj1.todense())\n",
    "adj1 = adj1.cuda()\n",
    "adj2 = torch.mm(adj1,adj1)\n",
    "adj3 = torch.mm(adj2,adj1)\n",
    "adj4 = torch.mm(adj3,adj1)\n",
    "adj5 = torch.mm(adj4,adj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.7897 acc_train: 0.1750 acc_val: 0.1820 acc_test: 0.2340 time: 0.1123s\n",
      "Epoch: 0002 loss_train: 1.7145 acc_train: 0.5250 acc_val: 0.4140 acc_test: 0.4070 time: 0.1056s\n",
      "Epoch: 0003 loss_train: 1.6246 acc_train: 0.8833 acc_val: 0.4400 acc_test: 0.4170 time: 0.1033s\n",
      "Epoch: 0004 loss_train: 1.5262 acc_train: 0.9417 acc_val: 0.4500 acc_test: 0.4050 time: 0.1028s\n",
      "Epoch: 0005 loss_train: 1.4554 acc_train: 0.9333 acc_val: 0.4520 acc_test: 0.4150 time: 0.1023s\n",
      "Epoch: 0006 loss_train: 1.3530 acc_train: 0.9583 acc_val: 0.4380 acc_test: 0.4250 time: 0.1023s\n",
      "Epoch: 0007 loss_train: 1.3017 acc_train: 0.9583 acc_val: 0.4260 acc_test: 0.4340 time: 0.1022s\n",
      "Epoch: 0008 loss_train: 1.1908 acc_train: 0.9917 acc_val: 0.4100 acc_test: 0.4130 time: 0.1023s\n",
      "Epoch: 0009 loss_train: 1.1181 acc_train: 0.9583 acc_val: 0.4120 acc_test: 0.4130 time: 0.1022s\n",
      "Epoch: 0010 loss_train: 1.0996 acc_train: 0.9667 acc_val: 0.4100 acc_test: 0.4100 time: 0.1024s\n",
      "Epoch: 0011 loss_train: 1.0294 acc_train: 0.9833 acc_val: 0.4320 acc_test: 0.4150 time: 0.1021s\n",
      "Epoch: 0012 loss_train: 0.9595 acc_train: 1.0000 acc_val: 0.4300 acc_test: 0.4140 time: 0.1024s\n",
      "Epoch: 0013 loss_train: 0.8946 acc_train: 0.9833 acc_val: 0.4200 acc_test: 0.4050 time: 0.1022s\n",
      "Epoch: 0014 loss_train: 0.8691 acc_train: 0.9750 acc_val: 0.4220 acc_test: 0.4020 time: 0.1023s\n",
      "Epoch: 0015 loss_train: 0.8615 acc_train: 0.9667 acc_val: 0.4220 acc_test: 0.4020 time: 0.1023s\n",
      "Epoch: 0016 loss_train: 0.8079 acc_train: 0.9917 acc_val: 0.4220 acc_test: 0.4020 time: 0.1023s\n",
      "Epoch: 0017 loss_train: 0.7260 acc_train: 1.0000 acc_val: 0.4240 acc_test: 0.4010 time: 0.1023s\n",
      "Epoch: 0018 loss_train: 0.7401 acc_train: 0.9917 acc_val: 0.4260 acc_test: 0.4040 time: 0.1024s\n",
      "Epoch: 0019 loss_train: 0.7822 acc_train: 0.9917 acc_val: 0.4160 acc_test: 0.4030 time: 0.1022s\n",
      "Epoch: 0020 loss_train: 0.7010 acc_train: 0.9917 acc_val: 0.4160 acc_test: 0.4050 time: 0.1024s\n",
      "Epoch: 0021 loss_train: 0.6615 acc_train: 0.9833 acc_val: 0.4080 acc_test: 0.4070 time: 0.1022s\n",
      "Epoch: 0022 loss_train: 0.6630 acc_train: 0.9667 acc_val: 0.3980 acc_test: 0.4020 time: 0.1024s\n",
      "Epoch: 0023 loss_train: 0.6660 acc_train: 0.9667 acc_val: 0.4040 acc_test: 0.4050 time: 0.1023s\n",
      "Epoch: 0024 loss_train: 0.6152 acc_train: 0.9583 acc_val: 0.4080 acc_test: 0.4060 time: 0.1023s\n",
      "Epoch: 0025 loss_train: 0.6525 acc_train: 0.9500 acc_val: 0.3960 acc_test: 0.4040 time: 0.1022s\n",
      "Epoch: 0026 loss_train: 0.5731 acc_train: 0.9833 acc_val: 0.3920 acc_test: 0.4010 time: 0.1024s\n",
      "Epoch: 0027 loss_train: 0.6018 acc_train: 0.9667 acc_val: 0.3860 acc_test: 0.4030 time: 0.1022s\n",
      "Epoch: 0028 loss_train: 0.5939 acc_train: 0.9833 acc_val: 0.4080 acc_test: 0.4080 time: 0.1024s\n",
      "Epoch: 0029 loss_train: 0.5660 acc_train: 0.9750 acc_val: 0.4260 acc_test: 0.4170 time: 0.1021s\n",
      "Epoch: 0030 loss_train: 0.5680 acc_train: 0.9833 acc_val: 0.4080 acc_test: 0.3980 time: 0.1024s\n",
      "Epoch: 0031 loss_train: 0.5473 acc_train: 1.0000 acc_val: 0.4040 acc_test: 0.3970 time: 0.1023s\n",
      "Epoch: 0032 loss_train: 0.6032 acc_train: 0.9500 acc_val: 0.3920 acc_test: 0.3930 time: 0.1024s\n",
      "Epoch: 0033 loss_train: 0.5528 acc_train: 0.9917 acc_val: 0.3940 acc_test: 0.3830 time: 0.1022s\n",
      "Epoch: 0034 loss_train: 0.5692 acc_train: 0.9833 acc_val: 0.3880 acc_test: 0.3830 time: 0.1024s\n",
      "Epoch: 0035 loss_train: 0.5930 acc_train: 0.9417 acc_val: 0.3900 acc_test: 0.3930 time: 0.1022s\n",
      "Epoch: 0036 loss_train: 0.5303 acc_train: 0.9750 acc_val: 0.4000 acc_test: 0.3910 time: 0.1022s\n",
      "Epoch: 0037 loss_train: 0.5377 acc_train: 0.9667 acc_val: 0.4280 acc_test: 0.3810 time: 0.1022s\n",
      "Epoch: 0038 loss_train: 0.5315 acc_train: 0.9917 acc_val: 0.4140 acc_test: 0.3810 time: 0.1024s\n",
      "Epoch: 0039 loss_train: 0.4850 acc_train: 0.9917 acc_val: 0.4060 acc_test: 0.3800 time: 0.1022s\n",
      "Epoch: 0040 loss_train: 0.5289 acc_train: 0.9917 acc_val: 0.4000 acc_test: 0.3810 time: 0.1023s\n",
      "Epoch: 0041 loss_train: 0.5030 acc_train: 0.9917 acc_val: 0.3880 acc_test: 0.3810 time: 0.1023s\n",
      "Epoch: 0042 loss_train: 0.5046 acc_train: 0.9833 acc_val: 0.3920 acc_test: 0.3790 time: 0.1023s\n",
      "Epoch: 0043 loss_train: 0.4967 acc_train: 0.9583 acc_val: 0.3900 acc_test: 0.3770 time: 0.1022s\n",
      "Epoch: 0044 loss_train: 0.5623 acc_train: 0.9750 acc_val: 0.3900 acc_test: 0.3730 time: 0.1022s\n",
      "Epoch: 0045 loss_train: 0.4234 acc_train: 0.9833 acc_val: 0.3940 acc_test: 0.3760 time: 0.1023s\n",
      "Epoch: 0046 loss_train: 0.4899 acc_train: 0.9667 acc_val: 0.3880 acc_test: 0.3730 time: 0.1024s\n",
      "Epoch: 0047 loss_train: 0.5299 acc_train: 0.9667 acc_val: 0.3880 acc_test: 0.3830 time: 0.1022s\n",
      "Epoch: 0048 loss_train: 0.5062 acc_train: 0.9833 acc_val: 0.3980 acc_test: 0.3870 time: 0.1025s\n",
      "Epoch: 0049 loss_train: 0.4757 acc_train: 0.9833 acc_val: 0.4020 acc_test: 0.3850 time: 0.1022s\n",
      "Epoch: 0050 loss_train: 0.4858 acc_train: 0.9917 acc_val: 0.4020 acc_test: 0.3860 time: 0.1024s\n",
      "Epoch: 0051 loss_train: 0.4790 acc_train: 0.9917 acc_val: 0.3980 acc_test: 0.3880 time: 0.1023s\n",
      "Epoch: 0052 loss_train: 0.5161 acc_train: 0.9833 acc_val: 0.4000 acc_test: 0.3920 time: 0.1024s\n",
      "Epoch: 0053 loss_train: 0.5263 acc_train: 0.9667 acc_val: 0.3940 acc_test: 0.4010 time: 0.1022s\n",
      "Epoch: 0054 loss_train: 0.4957 acc_train: 0.9750 acc_val: 0.4000 acc_test: 0.4080 time: 0.1023s\n",
      "Epoch: 0055 loss_train: 0.5122 acc_train: 0.9917 acc_val: 0.4180 acc_test: 0.4110 time: 0.1025s\n",
      "Epoch: 0056 loss_train: 0.4245 acc_train: 1.0000 acc_val: 0.4040 acc_test: 0.4140 time: 0.1022s\n",
      "Epoch: 0057 loss_train: 0.4530 acc_train: 0.9917 acc_val: 0.4120 acc_test: 0.4090 time: 0.1022s\n",
      "Epoch: 0058 loss_train: 0.5278 acc_train: 0.9750 acc_val: 0.4220 acc_test: 0.4020 time: 0.1024s\n",
      "Epoch: 0059 loss_train: 0.5033 acc_train: 0.9833 acc_val: 0.4100 acc_test: 0.3930 time: 0.1022s\n",
      "Epoch: 0060 loss_train: 0.4762 acc_train: 0.9583 acc_val: 0.4100 acc_test: 0.3900 time: 0.1023s\n",
      "Epoch: 0061 loss_train: 0.4727 acc_train: 0.9917 acc_val: 0.3960 acc_test: 0.3860 time: 0.1023s\n",
      "Epoch: 0062 loss_train: 0.4565 acc_train: 0.9750 acc_val: 0.3920 acc_test: 0.3870 time: 0.1023s\n",
      "Epoch: 0063 loss_train: 0.4983 acc_train: 0.9667 acc_val: 0.3880 acc_test: 0.3780 time: 0.1022s\n",
      "Epoch: 0064 loss_train: 0.5521 acc_train: 0.9583 acc_val: 0.3820 acc_test: 0.3740 time: 0.1023s\n",
      "Epoch: 0065 loss_train: 0.4560 acc_train: 1.0000 acc_val: 0.3840 acc_test: 0.3700 time: 0.1023s\n",
      "Epoch: 0066 loss_train: 0.4706 acc_train: 0.9750 acc_val: 0.3920 acc_test: 0.3690 time: 0.1024s\n",
      "Epoch: 0067 loss_train: 0.4892 acc_train: 0.9750 acc_val: 0.3960 acc_test: 0.3690 time: 0.1022s\n",
      "Epoch: 0068 loss_train: 0.4302 acc_train: 0.9750 acc_val: 0.3980 acc_test: 0.3800 time: 0.1024s\n",
      "Epoch: 0069 loss_train: 0.4978 acc_train: 0.9833 acc_val: 0.4020 acc_test: 0.3900 time: 0.1022s\n",
      "Epoch: 0070 loss_train: 0.4607 acc_train: 0.9917 acc_val: 0.4060 acc_test: 0.3980 time: 0.1023s\n",
      "Epoch: 0071 loss_train: 0.4672 acc_train: 0.9833 acc_val: 0.4220 acc_test: 0.3980 time: 0.1021s\n",
      "Epoch: 0072 loss_train: 0.5027 acc_train: 0.9833 acc_val: 0.4160 acc_test: 0.4090 time: 0.1024s\n",
      "Epoch: 0073 loss_train: 0.5329 acc_train: 0.9833 acc_val: 0.4120 acc_test: 0.4060 time: 0.1022s\n",
      "Epoch: 0074 loss_train: 0.4545 acc_train: 0.9833 acc_val: 0.4020 acc_test: 0.4020 time: 0.1024s\n",
      "Epoch: 0075 loss_train: 0.4400 acc_train: 0.9917 acc_val: 0.4020 acc_test: 0.4070 time: 0.1024s\n",
      "Epoch: 0076 loss_train: 0.5153 acc_train: 0.9917 acc_val: 0.4020 acc_test: 0.4050 time: 0.1024s\n",
      "Epoch: 0077 loss_train: 0.5200 acc_train: 0.9833 acc_val: 0.4220 acc_test: 0.4090 time: 0.1023s\n",
      "Epoch: 0078 loss_train: 0.4871 acc_train: 0.9583 acc_val: 0.4100 acc_test: 0.4000 time: 0.1027s\n",
      "Epoch: 0079 loss_train: 0.4426 acc_train: 0.9750 acc_val: 0.4020 acc_test: 0.3870 time: 0.1024s\n",
      "Epoch: 0080 loss_train: 0.4772 acc_train: 0.9833 acc_val: 0.4020 acc_test: 0.3780 time: 0.1024s\n",
      "Epoch: 0081 loss_train: 0.4226 acc_train: 0.9833 acc_val: 0.3880 acc_test: 0.3640 time: 0.1022s\n",
      "Epoch: 0082 loss_train: 0.4042 acc_train: 0.9833 acc_val: 0.3840 acc_test: 0.3640 time: 0.1024s\n",
      "Epoch: 0083 loss_train: 0.4958 acc_train: 0.9583 acc_val: 0.3760 acc_test: 0.3620 time: 0.1021s\n",
      "Epoch: 0084 loss_train: 0.3867 acc_train: 0.9750 acc_val: 0.3800 acc_test: 0.3660 time: 0.1024s\n",
      "Epoch: 0085 loss_train: 0.5261 acc_train: 0.9917 acc_val: 0.3840 acc_test: 0.3690 time: 0.1022s\n",
      "Epoch: 0086 loss_train: 0.4363 acc_train: 0.9667 acc_val: 0.3960 acc_test: 0.3810 time: 0.1023s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.3400 acc_train: 1.0000 acc_val: 0.3960 acc_test: 0.4060 time: 0.1022s\n",
      "Epoch: 0088 loss_train: 0.4315 acc_train: 0.9917 acc_val: 0.4120 acc_test: 0.4110 time: 0.1024s\n",
      "Epoch: 0089 loss_train: 0.5261 acc_train: 0.9667 acc_val: 0.4140 acc_test: 0.4030 time: 0.1021s\n",
      "Epoch: 0090 loss_train: 0.4679 acc_train: 0.9833 acc_val: 0.4140 acc_test: 0.4120 time: 0.1024s\n",
      "Epoch: 0091 loss_train: 0.4673 acc_train: 0.9917 acc_val: 0.4160 acc_test: 0.4180 time: 0.1021s\n",
      "Epoch: 0092 loss_train: 0.4786 acc_train: 1.0000 acc_val: 0.4080 acc_test: 0.4140 time: 0.1024s\n",
      "Epoch: 0093 loss_train: 0.4513 acc_train: 0.9750 acc_val: 0.3960 acc_test: 0.4210 time: 0.1023s\n",
      "Epoch: 0094 loss_train: 0.4543 acc_train: 0.9750 acc_val: 0.3980 acc_test: 0.4020 time: 0.1029s\n",
      "Epoch: 0095 loss_train: 0.3946 acc_train: 0.9917 acc_val: 0.3860 acc_test: 0.4000 time: 0.1023s\n",
      "Epoch: 0096 loss_train: 0.4457 acc_train: 1.0000 acc_val: 0.3900 acc_test: 0.3930 time: 0.1024s\n",
      "Epoch: 0097 loss_train: 0.4602 acc_train: 0.9833 acc_val: 0.4000 acc_test: 0.3960 time: 0.1023s\n",
      "Epoch: 0098 loss_train: 0.4140 acc_train: 0.9750 acc_val: 0.3980 acc_test: 0.4040 time: 0.1024s\n",
      "Epoch: 0099 loss_train: 0.4348 acc_train: 0.9917 acc_val: 0.3980 acc_test: 0.3890 time: 0.1022s\n",
      "Epoch: 0100 loss_train: 0.4138 acc_train: 1.0000 acc_val: 0.4040 acc_test: 0.3820 time: 0.1024s\n",
      "Epoch: 0101 loss_train: 0.4314 acc_train: 1.0000 acc_val: 0.3960 acc_test: 0.3810 time: 0.1022s\n",
      "Epoch: 0102 loss_train: 0.4402 acc_train: 0.9917 acc_val: 0.3940 acc_test: 0.3800 time: 0.1024s\n",
      "Epoch: 0103 loss_train: 0.4891 acc_train: 0.9667 acc_val: 0.3940 acc_test: 0.3680 time: 0.1022s\n",
      "Epoch: 0104 loss_train: 0.5156 acc_train: 0.9750 acc_val: 0.3880 acc_test: 0.3660 time: 0.1027s\n",
      "Epoch: 0105 loss_train: 0.4984 acc_train: 0.9667 acc_val: 0.3980 acc_test: 0.3650 time: 0.1023s\n",
      "Epoch: 0106 loss_train: 0.4612 acc_train: 0.9667 acc_val: 0.3980 acc_test: 0.3680 time: 0.1023s\n",
      "Epoch: 0107 loss_train: 0.4288 acc_train: 0.9750 acc_val: 0.4040 acc_test: 0.3720 time: 0.1021s\n",
      "Epoch: 0108 loss_train: 0.5336 acc_train: 0.9417 acc_val: 0.3980 acc_test: 0.3740 time: 0.1023s\n",
      "Epoch: 0109 loss_train: 0.4663 acc_train: 0.9917 acc_val: 0.4000 acc_test: 0.3840 time: 0.1022s\n",
      "Epoch: 0110 loss_train: 0.4818 acc_train: 0.9750 acc_val: 0.4080 acc_test: 0.3910 time: 0.1023s\n",
      "Epoch: 0111 loss_train: 0.3641 acc_train: 0.9917 acc_val: 0.4180 acc_test: 0.4000 time: 0.1022s\n",
      "Epoch: 0112 loss_train: 0.4319 acc_train: 0.9583 acc_val: 0.4080 acc_test: 0.4050 time: 0.1023s\n",
      "Epoch: 0113 loss_train: 0.3800 acc_train: 0.9833 acc_val: 0.3900 acc_test: 0.3980 time: 0.1021s\n",
      "Epoch: 0114 loss_train: 0.4243 acc_train: 0.9667 acc_val: 0.3840 acc_test: 0.3910 time: 0.1024s\n",
      "Epoch: 0115 loss_train: 0.4395 acc_train: 0.9667 acc_val: 0.3780 acc_test: 0.3950 time: 0.1021s\n",
      "Epoch: 0116 loss_train: 0.4854 acc_train: 0.9833 acc_val: 0.3860 acc_test: 0.3960 time: 0.1023s\n",
      "Epoch: 0117 loss_train: 0.4496 acc_train: 0.9667 acc_val: 0.3960 acc_test: 0.3950 time: 0.1021s\n",
      "Epoch: 0118 loss_train: 0.4403 acc_train: 0.9833 acc_val: 0.4060 acc_test: 0.3900 time: 0.1023s\n",
      "Epoch: 0119 loss_train: 0.4976 acc_train: 0.9750 acc_val: 0.4140 acc_test: 0.3920 time: 0.1022s\n",
      "Epoch: 0120 loss_train: 0.3681 acc_train: 0.9917 acc_val: 0.4080 acc_test: 0.3930 time: 0.1023s\n",
      "Epoch: 0121 loss_train: 0.4179 acc_train: 0.9917 acc_val: 0.4120 acc_test: 0.3920 time: 0.1023s\n",
      "Epoch: 0122 loss_train: 0.4094 acc_train: 0.9833 acc_val: 0.4160 acc_test: 0.3960 time: 0.1024s\n",
      "Epoch: 0123 loss_train: 0.4871 acc_train: 0.9667 acc_val: 0.4180 acc_test: 0.4110 time: 0.1021s\n",
      "Epoch: 0124 loss_train: 0.4595 acc_train: 0.9917 acc_val: 0.4160 acc_test: 0.4190 time: 0.1024s\n",
      "Epoch: 0125 loss_train: 0.4554 acc_train: 0.9667 acc_val: 0.4160 acc_test: 0.4220 time: 0.1023s\n",
      "Epoch: 0126 loss_train: 0.4389 acc_train: 0.9750 acc_val: 0.4140 acc_test: 0.4200 time: 0.1024s\n",
      "Epoch: 0127 loss_train: 0.5022 acc_train: 0.9750 acc_val: 0.4260 acc_test: 0.4070 time: 0.1022s\n",
      "Epoch: 0128 loss_train: 0.3834 acc_train: 0.9750 acc_val: 0.4040 acc_test: 0.3830 time: 0.1024s\n",
      "Epoch: 0129 loss_train: 0.4240 acc_train: 0.9833 acc_val: 0.3960 acc_test: 0.3770 time: 0.1022s\n",
      "Epoch: 0130 loss_train: 0.3888 acc_train: 0.9917 acc_val: 0.3900 acc_test: 0.3790 time: 0.1024s\n",
      "Epoch: 0131 loss_train: 0.4412 acc_train: 0.9583 acc_val: 0.3820 acc_test: 0.3770 time: 0.1022s\n",
      "Epoch: 0132 loss_train: 0.4489 acc_train: 0.9750 acc_val: 0.3880 acc_test: 0.3680 time: 0.1024s\n",
      "Epoch: 0133 loss_train: 0.4112 acc_train: 0.9833 acc_val: 0.3820 acc_test: 0.3750 time: 0.1023s\n",
      "Epoch: 0134 loss_train: 0.4701 acc_train: 0.9833 acc_val: 0.3820 acc_test: 0.3730 time: 0.1024s\n",
      "Epoch: 0135 loss_train: 0.5228 acc_train: 0.9583 acc_val: 0.3940 acc_test: 0.3730 time: 0.1021s\n",
      "Epoch: 0136 loss_train: 0.4182 acc_train: 0.9833 acc_val: 0.3900 acc_test: 0.3750 time: 0.1026s\n",
      "Epoch: 0137 loss_train: 0.4394 acc_train: 0.9917 acc_val: 0.3900 acc_test: 0.3790 time: 0.1023s\n",
      "Epoch: 0138 loss_train: 0.3927 acc_train: 0.9917 acc_val: 0.4000 acc_test: 0.3830 time: 0.1024s\n",
      "Epoch: 0139 loss_train: 0.4195 acc_train: 0.9917 acc_val: 0.4080 acc_test: 0.3840 time: 0.1023s\n",
      "Epoch: 0140 loss_train: 0.3962 acc_train: 0.9750 acc_val: 0.4220 acc_test: 0.3860 time: 0.1024s\n",
      "Epoch: 0141 loss_train: 0.4192 acc_train: 0.9917 acc_val: 0.4080 acc_test: 0.3830 time: 0.1023s\n",
      "Epoch: 0142 loss_train: 0.3917 acc_train: 0.9583 acc_val: 0.4000 acc_test: 0.3790 time: 0.1024s\n",
      "Epoch: 0143 loss_train: 0.4835 acc_train: 0.9917 acc_val: 0.4020 acc_test: 0.3780 time: 0.1022s\n",
      "Epoch: 0144 loss_train: 0.4488 acc_train: 0.9750 acc_val: 0.3980 acc_test: 0.3720 time: 0.1023s\n",
      "Epoch: 0145 loss_train: 0.3707 acc_train: 0.9917 acc_val: 0.4140 acc_test: 0.3800 time: 0.1022s\n",
      "Epoch: 0146 loss_train: 0.4195 acc_train: 0.9750 acc_val: 0.4140 acc_test: 0.3920 time: 0.1023s\n",
      "Epoch: 0147 loss_train: 0.4200 acc_train: 0.9750 acc_val: 0.4080 acc_test: 0.3930 time: 0.1021s\n",
      "Epoch: 0148 loss_train: 0.3889 acc_train: 0.9667 acc_val: 0.4020 acc_test: 0.3970 time: 0.1023s\n",
      "Epoch: 0149 loss_train: 0.4662 acc_train: 0.9833 acc_val: 0.4200 acc_test: 0.3970 time: 0.1021s\n",
      "Epoch: 0150 loss_train: 0.5215 acc_train: 0.9417 acc_val: 0.3980 acc_test: 0.4020 time: 0.1024s\n",
      "Epoch: 0151 loss_train: 0.5125 acc_train: 0.9750 acc_val: 0.4160 acc_test: 0.4180 time: 0.1022s\n",
      "Epoch: 0152 loss_train: 0.4998 acc_train: 0.9833 acc_val: 0.4100 acc_test: 0.4280 time: 0.1024s\n",
      "Epoch: 0153 loss_train: 0.5013 acc_train: 0.9833 acc_val: 0.4240 acc_test: 0.4300 time: 0.1023s\n",
      "Epoch: 0154 loss_train: 0.4205 acc_train: 0.9833 acc_val: 0.3960 acc_test: 0.4140 time: 0.1023s\n",
      "Epoch: 0155 loss_train: 0.4417 acc_train: 0.9750 acc_val: 0.3880 acc_test: 0.3970 time: 0.1022s\n",
      "Epoch: 0156 loss_train: 0.4128 acc_train: 0.9833 acc_val: 0.3960 acc_test: 0.3850 time: 0.1024s\n",
      "Epoch: 0157 loss_train: 0.4041 acc_train: 0.9833 acc_val: 0.3980 acc_test: 0.3820 time: 0.1022s\n",
      "Epoch: 0158 loss_train: 0.4012 acc_train: 0.9833 acc_val: 0.3980 acc_test: 0.3800 time: 0.1023s\n",
      "Epoch: 0159 loss_train: 0.4099 acc_train: 0.9917 acc_val: 0.3980 acc_test: 0.3790 time: 0.1021s\n",
      "Epoch: 0160 loss_train: 0.4420 acc_train: 0.9750 acc_val: 0.4000 acc_test: 0.3820 time: 0.1024s\n",
      "Epoch: 0161 loss_train: 0.4302 acc_train: 0.9833 acc_val: 0.4000 acc_test: 0.3800 time: 0.1022s\n",
      "Epoch: 0162 loss_train: 0.4029 acc_train: 0.9833 acc_val: 0.4040 acc_test: 0.3810 time: 0.1023s\n",
      "Epoch: 0163 loss_train: 0.4036 acc_train: 0.9750 acc_val: 0.4140 acc_test: 0.3900 time: 0.1023s\n",
      "Epoch: 0164 loss_train: 0.4566 acc_train: 0.9417 acc_val: 0.4100 acc_test: 0.4000 time: 0.1024s\n",
      "Epoch: 0165 loss_train: 0.3693 acc_train: 0.9833 acc_val: 0.4000 acc_test: 0.3960 time: 0.1021s\n",
      "Epoch: 0166 loss_train: 0.4408 acc_train: 0.9583 acc_val: 0.4000 acc_test: 0.3980 time: 0.1022s\n",
      "Epoch: 0167 loss_train: 0.3892 acc_train: 0.9833 acc_val: 0.3900 acc_test: 0.4000 time: 0.1022s\n",
      "Epoch: 0168 loss_train: 0.3953 acc_train: 0.9750 acc_val: 0.3920 acc_test: 0.4050 time: 0.1023s\n",
      "Epoch: 0169 loss_train: 0.3960 acc_train: 0.9667 acc_val: 0.4000 acc_test: 0.4150 time: 0.1022s\n",
      "Epoch: 0170 loss_train: 0.3546 acc_train: 1.0000 acc_val: 0.4360 acc_test: 0.4440 time: 0.1025s\n",
      "Epoch: 0171 loss_train: 0.3996 acc_train: 0.9750 acc_val: 0.4480 acc_test: 0.4460 time: 0.1024s\n",
      "Epoch: 0172 loss_train: 0.4210 acc_train: 0.9833 acc_val: 0.4480 acc_test: 0.4490 time: 0.1025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0173 loss_train: 0.4765 acc_train: 0.9750 acc_val: 0.4000 acc_test: 0.3940 time: 0.1023s\n",
      "Epoch: 0174 loss_train: 0.3922 acc_train: 0.9750 acc_val: 0.3980 acc_test: 0.3770 time: 0.1023s\n",
      "Epoch: 0175 loss_train: 0.4083 acc_train: 0.9917 acc_val: 0.3820 acc_test: 0.3650 time: 0.1022s\n",
      "Epoch: 0176 loss_train: 0.4158 acc_train: 0.9750 acc_val: 0.3880 acc_test: 0.3580 time: 0.1024s\n",
      "Epoch: 0177 loss_train: 0.4295 acc_train: 0.9583 acc_val: 0.3820 acc_test: 0.3550 time: 0.1022s\n",
      "Epoch: 0178 loss_train: 0.4542 acc_train: 0.9583 acc_val: 0.3780 acc_test: 0.3570 time: 0.1023s\n",
      "Epoch: 0179 loss_train: 0.4052 acc_train: 0.9583 acc_val: 0.3820 acc_test: 0.3570 time: 0.1023s\n",
      "Epoch: 0180 loss_train: 0.4183 acc_train: 0.9667 acc_val: 0.3820 acc_test: 0.3640 time: 0.1023s\n",
      "Epoch: 0181 loss_train: 0.3924 acc_train: 0.9833 acc_val: 0.3980 acc_test: 0.3730 time: 0.1022s\n",
      "Epoch: 0182 loss_train: 0.4115 acc_train: 0.9750 acc_val: 0.4100 acc_test: 0.3780 time: 0.1026s\n",
      "Epoch: 0183 loss_train: 0.3875 acc_train: 0.9667 acc_val: 0.4120 acc_test: 0.3820 time: 0.1023s\n",
      "Epoch: 0184 loss_train: 0.4082 acc_train: 0.9833 acc_val: 0.4180 acc_test: 0.3880 time: 0.1024s\n",
      "Epoch: 0185 loss_train: 0.4587 acc_train: 0.9833 acc_val: 0.4200 acc_test: 0.3880 time: 0.1022s\n",
      "Epoch: 0186 loss_train: 0.4284 acc_train: 0.9917 acc_val: 0.4160 acc_test: 0.3850 time: 0.1024s\n",
      "Epoch: 0187 loss_train: 0.4262 acc_train: 0.9833 acc_val: 0.4040 acc_test: 0.3820 time: 0.1022s\n",
      "Epoch: 0188 loss_train: 0.4110 acc_train: 0.9667 acc_val: 0.4000 acc_test: 0.3840 time: 0.1024s\n",
      "Epoch: 0189 loss_train: 0.4910 acc_train: 0.9583 acc_val: 0.4060 acc_test: 0.3890 time: 0.1022s\n",
      "Epoch: 0190 loss_train: 0.4225 acc_train: 0.9833 acc_val: 0.4100 acc_test: 0.3960 time: 0.1022s\n",
      "Epoch: 0191 loss_train: 0.4128 acc_train: 0.9917 acc_val: 0.4280 acc_test: 0.4100 time: 0.1023s\n",
      "Epoch: 0192 loss_train: 0.4382 acc_train: 0.9583 acc_val: 0.3960 acc_test: 0.3950 time: 0.1024s\n",
      "Epoch: 0193 loss_train: 0.4341 acc_train: 0.9833 acc_val: 0.3900 acc_test: 0.3970 time: 0.1023s\n",
      "Epoch: 0194 loss_train: 0.3686 acc_train: 1.0000 acc_val: 0.3800 acc_test: 0.4000 time: 0.1024s\n",
      "Epoch: 0195 loss_train: 0.4689 acc_train: 0.9667 acc_val: 0.4100 acc_test: 0.4370 time: 0.1024s\n",
      "Epoch: 0196 loss_train: 0.4309 acc_train: 0.9833 acc_val: 0.3920 acc_test: 0.4170 time: 0.1023s\n",
      "Epoch: 0197 loss_train: 0.4125 acc_train: 0.9667 acc_val: 0.3880 acc_test: 0.3970 time: 0.1022s\n",
      "Epoch: 0198 loss_train: 0.4427 acc_train: 0.9667 acc_val: 0.3880 acc_test: 0.3800 time: 0.1023s\n",
      "Epoch: 0199 loss_train: 0.4414 acc_train: 0.9750 acc_val: 0.3920 acc_test: 0.3770 time: 0.1022s\n",
      "Epoch: 0200 loss_train: 0.4150 acc_train: 0.9583 acc_val: 0.3960 acc_test: 0.3690 time: 0.1024s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 20.6166s\n",
      "0.452 0.41500000000000004\n",
      "0.45 0.405\n",
      "0.448 0.449\n",
      "0.44 0.417\n",
      "0.438 0.425\n",
      "0.436 0.444\n",
      "0.432 0.41500000000000004\n",
      "0.43 0.41400000000000003\n",
      "0.428 0.41000000000000003\n",
      "0.426 0.40700000000000003\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN(nfeat=adj.shape[1],\n",
    "        nclass=labels.max().item() + 1,\n",
    "        dropout=0.5)\n",
    "model1.cuda()\n",
    "optimizer = optim.Adam(model1.parameters(),\n",
    "                       lr=0.02, weight_decay=5e-4)\n",
    "t_total = time.time()\n",
    "record = {}\n",
    "for epoch in range(200):\n",
    "    train(epoch,model1,record,features,adj1,adj2,adj3,adj4,adj5)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "bit_list = sorted(record.keys())\n",
    "bit_list.reverse()\n",
    "for key in bit_list[:10]:\n",
    "    value = record[key]\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. P2CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = A_adj+np.eye(A_adj.shape[0])\n",
    "A2 = B_adj+np.eye(B_adj.shape[0])\n",
    "A1 = A_adj\n",
    "A1_2 = A1*A1\n",
    "A1_3 = A1*A1*A1\n",
    "A1_4 = A1*A1*A1*A1\n",
    "\n",
    "A2 = B_adj\n",
    "A2_2 = A2*A2\n",
    "A2_3 = A2*A2*A2\n",
    "A2_4 = A2*A2*A2*A2\n",
    "DP_A1 = DP(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2_5 = A2*A2*A2*A2*A2\n",
    "Norm_adj = normalize(adj)\n",
    "Norm_adj_2 = Norm_adj*Norm_adj\n",
    "Norm_adj_3 = Norm_adj_2*Norm_adj\n",
    "Norm_adj_4 = Norm_adj_3*Norm_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_adj_new = A_adj + DP(A2)\n",
    "A_adj_sparse = sparse.csr_matrix(A_adj_new)\n",
    "adj1 = normalize_adj(A_adj_sparse + sp.eye(A_adj.shape[0]))\n",
    "adj1 = sparse_mx_to_torch_sparse_tensor(adj1).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1 = DP_A1 - A1\n",
    "B_new= DP(2*A2)\n",
    "theta_2 = B_new - 2*A2\n",
    "A_3 = 2*DP_A1*A2+A2_2\n",
    "theta_3 = DP(A_3)-A_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1= Norm_adj+ theta_3 - theta_1*theta_2\n",
    "t2 = adj + theta_2\n",
    "A_adj_new2 = t1*t2\n",
    "A_adj_new2[A_adj_new2<0]=0\n",
    "A_adj_sparse = sparse.csr_matrix(A_adj_new2)\n",
    "adj2 = normalize_adj(A_adj_sparse + sp.eye(A_adj_new2.shape[0]))\n",
    "adj2 = sparse_mx_to_torch_sparse_tensor(adj2).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_new= DP(3*A2+3*A2_2)\n",
    "theta_2 = B_new - (3*A2+3*A2_2)\n",
    "A_3 = 3*DP_A1*A2_2+3*(A1_2+theta_1)*A2+A2_3\n",
    "theta_3 = DP(A_3)-A_3\n",
    "\n",
    "t1= Norm_adj_2+ theta_3 - theta_1*theta_2\n",
    "t2 = adj + theta_2\n",
    "A_adj_new3 = t1*t2\n",
    "A_adj_new3[A_adj_new3<0]=0\n",
    "A_adj_sparse = sparse.csr_matrix(A_adj_new3)\n",
    "adj3 = normalize_adj(A_adj_sparse + sp.eye(A_adj_new3.shape[0]))\n",
    "adj3 = sparse_mx_to_torch_sparse_tensor(adj3).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_new= DP(4*A2_3+6*A2_2+4*A2)\n",
    "theta_2 = B_new - (4*A2_3+6*A2_2+4*A2)\n",
    "A_3 = 4*DP_A1*A2_3+6*(A1_2+theta_1)*A2_2+4*(A1_3+theta_1)*A2+A2_4\n",
    "theta_3 = DP(A_3)-A_3\n",
    "\n",
    "t1= Norm_adj_3+ theta_3 - theta_1*theta_2\n",
    "t2 = adj + theta_2\n",
    "A_adj_new4 = t1*t2\n",
    "A_adj_new4[A_adj_new4<0]=0\n",
    "A_adj_sparse = sparse.csr_matrix(A_adj_new4)\n",
    "adj4 = normalize_adj(A_adj_sparse + sp.eye(A_adj_new4.shape[0]))\n",
    "adj4 = sparse_mx_to_torch_sparse_tensor(adj4).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = A_adj\n",
    "A2 = B_adj\n",
    "B_new= DP(5*A2_4+10*A2_3+10*A2_2+5*A2)\n",
    "theta_2 = B_new - (5*A2_4+10*A2_3+10*A2_2+5*A2)\n",
    "A_3 = 5*DP_A1*A2_4+10*(A1_2+theta_1)*A2_3+10*(A1_3+theta_1)*A2_2+5*(A1_4+theta_1)*A2+A2_5\n",
    "theta_3 = DP(A_3)-A_3\n",
    "\n",
    "t1= Norm_adj_4+ theta_3 - theta_1*theta_2\n",
    "t2 = adj + theta_2\n",
    "A_adj_new5 = t1*t2\n",
    "A_adj_new5[A_adj_new5<0]=0\n",
    "A_adj_sparse = sparse.csr_matrix(A_adj_new5)\n",
    "adj5 = normalize_adj(A_adj_sparse + sp.eye(A_adj_new5.shape[0]))\n",
    "adj5 = sparse_mx_to_torch_sparse_tensor(adj5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.7916 acc_train: 0.2167 acc_val: 0.2940 acc_test: 0.2790 time: 0.0554s\n",
      "Epoch: 0002 loss_train: 1.6949 acc_train: 0.3500 acc_val: 0.3820 acc_test: 0.3820 time: 0.0560s\n",
      "Epoch: 0003 loss_train: 1.5497 acc_train: 0.6917 acc_val: 0.3440 acc_test: 0.3460 time: 0.0554s\n",
      "Epoch: 0004 loss_train: 1.3710 acc_train: 0.8250 acc_val: 0.3200 acc_test: 0.3290 time: 0.0558s\n",
      "Epoch: 0005 loss_train: 1.3346 acc_train: 0.7750 acc_val: 0.3460 acc_test: 0.3570 time: 0.0555s\n",
      "Epoch: 0006 loss_train: 1.2270 acc_train: 0.8250 acc_val: 0.3720 acc_test: 0.3990 time: 0.0557s\n",
      "Epoch: 0007 loss_train: 1.1369 acc_train: 0.8917 acc_val: 0.4060 acc_test: 0.4170 time: 0.0558s\n",
      "Epoch: 0008 loss_train: 0.9705 acc_train: 0.9500 acc_val: 0.3980 acc_test: 0.3900 time: 0.0554s\n",
      "Epoch: 0009 loss_train: 0.9515 acc_train: 0.9167 acc_val: 0.3720 acc_test: 0.3740 time: 0.0558s\n",
      "Epoch: 0010 loss_train: 0.8678 acc_train: 0.8667 acc_val: 0.3840 acc_test: 0.3780 time: 0.0556s\n",
      "Epoch: 0011 loss_train: 0.7366 acc_train: 0.9083 acc_val: 0.4000 acc_test: 0.3820 time: 0.0557s\n",
      "Epoch: 0012 loss_train: 0.7914 acc_train: 0.8750 acc_val: 0.4000 acc_test: 0.3940 time: 0.0558s\n",
      "Epoch: 0013 loss_train: 0.5991 acc_train: 0.9583 acc_val: 0.4060 acc_test: 0.4050 time: 0.0554s\n",
      "Epoch: 0014 loss_train: 0.6329 acc_train: 0.9500 acc_val: 0.3920 acc_test: 0.4160 time: 0.0557s\n",
      "Epoch: 0015 loss_train: 0.6291 acc_train: 0.9667 acc_val: 0.3940 acc_test: 0.4150 time: 0.0555s\n",
      "Epoch: 0016 loss_train: 0.6439 acc_train: 0.9500 acc_val: 0.3880 acc_test: 0.4150 time: 0.0559s\n",
      "Epoch: 0017 loss_train: 0.6622 acc_train: 0.9750 acc_val: 0.3960 acc_test: 0.4160 time: 0.0556s\n",
      "Epoch: 0018 loss_train: 0.5807 acc_train: 0.9750 acc_val: 0.4140 acc_test: 0.3990 time: 0.0554s\n",
      "Epoch: 0019 loss_train: 0.5842 acc_train: 0.9750 acc_val: 0.4120 acc_test: 0.3920 time: 0.0579s\n",
      "Epoch: 0020 loss_train: 0.5989 acc_train: 0.9333 acc_val: 0.4160 acc_test: 0.3910 time: 0.0554s\n",
      "Epoch: 0021 loss_train: 0.5788 acc_train: 0.9500 acc_val: 0.4060 acc_test: 0.4210 time: 0.0559s\n",
      "Epoch: 0022 loss_train: 0.6086 acc_train: 0.9417 acc_val: 0.3980 acc_test: 0.4200 time: 0.0559s\n",
      "Epoch: 0023 loss_train: 0.5034 acc_train: 0.9750 acc_val: 0.4000 acc_test: 0.4180 time: 0.0554s\n",
      "Epoch: 0024 loss_train: 0.5050 acc_train: 0.9417 acc_val: 0.4080 acc_test: 0.4220 time: 0.0558s\n",
      "Epoch: 0025 loss_train: 0.5380 acc_train: 0.9583 acc_val: 0.3960 acc_test: 0.4260 time: 0.0555s\n",
      "Epoch: 0026 loss_train: 0.4732 acc_train: 0.9667 acc_val: 0.4020 acc_test: 0.4200 time: 0.0554s\n",
      "Epoch: 0027 loss_train: 0.6402 acc_train: 0.9250 acc_val: 0.4000 acc_test: 0.4080 time: 0.0558s\n",
      "Epoch: 0028 loss_train: 0.5666 acc_train: 0.9750 acc_val: 0.3940 acc_test: 0.3870 time: 0.0554s\n",
      "Epoch: 0029 loss_train: 0.5783 acc_train: 0.9333 acc_val: 0.3780 acc_test: 0.3630 time: 0.0558s\n",
      "Epoch: 0030 loss_train: 0.6079 acc_train: 0.9083 acc_val: 0.3700 acc_test: 0.3550 time: 0.0554s\n",
      "Epoch: 0031 loss_train: 0.4774 acc_train: 0.9500 acc_val: 0.3740 acc_test: 0.3540 time: 0.0554s\n",
      "Epoch: 0032 loss_train: 0.5812 acc_train: 0.9250 acc_val: 0.3860 acc_test: 0.3740 time: 0.0557s\n",
      "Epoch: 0033 loss_train: 0.5920 acc_train: 0.9500 acc_val: 0.3920 acc_test: 0.3940 time: 0.0553s\n",
      "Epoch: 0034 loss_train: 0.5733 acc_train: 0.9250 acc_val: 0.3900 acc_test: 0.4080 time: 0.0558s\n",
      "Epoch: 0035 loss_train: 0.5723 acc_train: 0.9333 acc_val: 0.3900 acc_test: 0.4140 time: 0.0554s\n",
      "Epoch: 0036 loss_train: 0.4865 acc_train: 0.9583 acc_val: 0.3840 acc_test: 0.4040 time: 0.0558s\n",
      "Epoch: 0037 loss_train: 0.5980 acc_train: 0.9333 acc_val: 0.3800 acc_test: 0.4100 time: 0.0554s\n",
      "Epoch: 0038 loss_train: 0.5736 acc_train: 0.9667 acc_val: 0.3820 acc_test: 0.4210 time: 0.0555s\n",
      "Epoch: 0039 loss_train: 0.5262 acc_train: 0.9917 acc_val: 0.3940 acc_test: 0.4150 time: 0.0559s\n",
      "Epoch: 0040 loss_train: 0.6306 acc_train: 0.9250 acc_val: 0.4020 acc_test: 0.4380 time: 0.0554s\n",
      "Epoch: 0041 loss_train: 0.5566 acc_train: 0.9833 acc_val: 0.4280 acc_test: 0.4500 time: 0.0559s\n",
      "Epoch: 0042 loss_train: 0.4967 acc_train: 0.9667 acc_val: 0.4200 acc_test: 0.4490 time: 0.0554s\n",
      "Epoch: 0043 loss_train: 0.5737 acc_train: 0.9417 acc_val: 0.4220 acc_test: 0.4550 time: 0.0555s\n",
      "Epoch: 0044 loss_train: 0.6144 acc_train: 0.9250 acc_val: 0.4300 acc_test: 0.4570 time: 0.0559s\n",
      "Epoch: 0045 loss_train: 0.5476 acc_train: 0.9417 acc_val: 0.4420 acc_test: 0.4640 time: 0.0554s\n",
      "Epoch: 0046 loss_train: 0.5236 acc_train: 0.9583 acc_val: 0.4200 acc_test: 0.4510 time: 0.0560s\n",
      "Epoch: 0047 loss_train: 0.5337 acc_train: 0.9583 acc_val: 0.4000 acc_test: 0.4050 time: 0.0553s\n",
      "Epoch: 0048 loss_train: 0.5984 acc_train: 0.9833 acc_val: 0.3860 acc_test: 0.3950 time: 0.0556s\n",
      "Epoch: 0049 loss_train: 0.4454 acc_train: 0.9750 acc_val: 0.3840 acc_test: 0.3930 time: 0.0558s\n",
      "Epoch: 0050 loss_train: 0.5556 acc_train: 0.9583 acc_val: 0.3780 acc_test: 0.3910 time: 0.0556s\n",
      "Epoch: 0051 loss_train: 0.5797 acc_train: 0.9417 acc_val: 0.3820 acc_test: 0.3890 time: 0.0557s\n",
      "Epoch: 0052 loss_train: 0.5530 acc_train: 0.9250 acc_val: 0.3880 acc_test: 0.3950 time: 0.0554s\n",
      "Epoch: 0053 loss_train: 0.5819 acc_train: 0.9250 acc_val: 0.4120 acc_test: 0.4170 time: 0.0557s\n",
      "Epoch: 0054 loss_train: 0.5736 acc_train: 0.9500 acc_val: 0.4300 acc_test: 0.4500 time: 0.0558s\n",
      "Epoch: 0055 loss_train: 0.5739 acc_train: 0.9167 acc_val: 0.4080 acc_test: 0.4400 time: 0.0553s\n",
      "Epoch: 0056 loss_train: 0.5406 acc_train: 0.9250 acc_val: 0.3780 acc_test: 0.3660 time: 0.0557s\n",
      "Epoch: 0057 loss_train: 0.4489 acc_train: 0.9667 acc_val: 0.3640 acc_test: 0.3570 time: 0.0554s\n",
      "Epoch: 0058 loss_train: 0.5769 acc_train: 0.9333 acc_val: 0.3620 acc_test: 0.3460 time: 0.0559s\n",
      "Epoch: 0059 loss_train: 0.5676 acc_train: 0.9167 acc_val: 0.3680 acc_test: 0.3490 time: 0.0553s\n",
      "Epoch: 0060 loss_train: 0.5680 acc_train: 0.9250 acc_val: 0.3800 acc_test: 0.3680 time: 0.0554s\n",
      "Epoch: 0061 loss_train: 0.4983 acc_train: 0.9500 acc_val: 0.4100 acc_test: 0.4350 time: 0.0558s\n",
      "Epoch: 0062 loss_train: 0.4874 acc_train: 0.9500 acc_val: 0.4260 acc_test: 0.4440 time: 0.0554s\n",
      "Epoch: 0063 loss_train: 0.5768 acc_train: 0.9333 acc_val: 0.3980 acc_test: 0.4140 time: 0.0558s\n",
      "Epoch: 0064 loss_train: 0.5656 acc_train: 0.9583 acc_val: 0.3720 acc_test: 0.3990 time: 0.0555s\n",
      "Epoch: 0065 loss_train: 0.5869 acc_train: 0.9500 acc_val: 0.3660 acc_test: 0.3950 time: 0.0556s\n",
      "Epoch: 0066 loss_train: 0.5946 acc_train: 0.9333 acc_val: 0.3840 acc_test: 0.3930 time: 0.0558s\n",
      "Epoch: 0067 loss_train: 0.6180 acc_train: 0.9333 acc_val: 0.3760 acc_test: 0.3910 time: 0.0554s\n",
      "Epoch: 0068 loss_train: 0.5638 acc_train: 0.9333 acc_val: 0.3620 acc_test: 0.3920 time: 0.0557s\n",
      "Epoch: 0069 loss_train: 0.5736 acc_train: 0.9417 acc_val: 0.3640 acc_test: 0.3910 time: 0.0554s\n",
      "Epoch: 0070 loss_train: 0.5594 acc_train: 0.9250 acc_val: 0.3820 acc_test: 0.4100 time: 0.0582s\n",
      "Epoch: 0071 loss_train: 0.5158 acc_train: 0.9417 acc_val: 0.3900 acc_test: 0.3950 time: 0.0559s\n",
      "Epoch: 0072 loss_train: 0.5030 acc_train: 0.9583 acc_val: 0.3700 acc_test: 0.3820 time: 0.0555s\n",
      "Epoch: 0073 loss_train: 0.6056 acc_train: 0.9083 acc_val: 0.3620 acc_test: 0.3710 time: 0.0566s\n",
      "Epoch: 0074 loss_train: 0.7117 acc_train: 0.8833 acc_val: 0.3640 acc_test: 0.3780 time: 0.0559s\n",
      "Epoch: 0075 loss_train: 0.6474 acc_train: 0.9333 acc_val: 0.3800 acc_test: 0.3890 time: 0.0553s\n",
      "Epoch: 0076 loss_train: 0.5055 acc_train: 0.9500 acc_val: 0.4020 acc_test: 0.4200 time: 0.0558s\n",
      "Epoch: 0077 loss_train: 0.6172 acc_train: 0.9500 acc_val: 0.3940 acc_test: 0.3940 time: 0.0555s\n",
      "Epoch: 0078 loss_train: 0.5378 acc_train: 0.9333 acc_val: 0.3520 acc_test: 0.3560 time: 0.0556s\n",
      "Epoch: 0079 loss_train: 0.5513 acc_train: 0.9250 acc_val: 0.3460 acc_test: 0.3520 time: 0.0557s\n",
      "Epoch: 0080 loss_train: 0.5674 acc_train: 0.9000 acc_val: 0.3400 acc_test: 0.3600 time: 0.0553s\n",
      "Epoch: 0081 loss_train: 0.5717 acc_train: 0.9000 acc_val: 0.3480 acc_test: 0.3730 time: 0.0559s\n",
      "Epoch: 0082 loss_train: 0.5148 acc_train: 0.9083 acc_val: 0.3440 acc_test: 0.3750 time: 0.0555s\n",
      "Epoch: 0083 loss_train: 0.5381 acc_train: 0.9167 acc_val: 0.3720 acc_test: 0.3860 time: 0.0557s\n",
      "Epoch: 0084 loss_train: 0.5835 acc_train: 0.9000 acc_val: 0.3860 acc_test: 0.4100 time: 0.0556s\n",
      "Epoch: 0085 loss_train: 0.4573 acc_train: 0.9500 acc_val: 0.3980 acc_test: 0.4260 time: 0.0554s\n",
      "Epoch: 0086 loss_train: 0.4591 acc_train: 0.9500 acc_val: 0.3880 acc_test: 0.4120 time: 0.0559s\n",
      "Epoch: 0087 loss_train: 0.5564 acc_train: 0.9500 acc_val: 0.3680 acc_test: 0.3960 time: 0.0555s\n",
      "Epoch: 0088 loss_train: 0.5679 acc_train: 0.9167 acc_val: 0.3560 acc_test: 0.3830 time: 0.0559s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0089 loss_train: 0.5297 acc_train: 0.9583 acc_val: 0.3540 acc_test: 0.3860 time: 0.0555s\n",
      "Epoch: 0090 loss_train: 0.5697 acc_train: 0.9083 acc_val: 0.3680 acc_test: 0.3960 time: 0.0555s\n",
      "Epoch: 0091 loss_train: 0.4108 acc_train: 0.9667 acc_val: 0.3900 acc_test: 0.4140 time: 0.0559s\n",
      "Epoch: 0092 loss_train: 0.5048 acc_train: 0.9667 acc_val: 0.4060 acc_test: 0.4210 time: 0.0553s\n",
      "Epoch: 0093 loss_train: 0.5639 acc_train: 0.9667 acc_val: 0.4040 acc_test: 0.4250 time: 0.0559s\n",
      "Epoch: 0094 loss_train: 0.6481 acc_train: 0.9167 acc_val: 0.3900 acc_test: 0.4130 time: 0.0574s\n",
      "Epoch: 0095 loss_train: 0.5527 acc_train: 0.9500 acc_val: 0.3800 acc_test: 0.4070 time: 0.0555s\n",
      "Epoch: 0096 loss_train: 0.5255 acc_train: 0.9333 acc_val: 0.3740 acc_test: 0.4050 time: 0.0559s\n",
      "Epoch: 0097 loss_train: 0.5367 acc_train: 0.9333 acc_val: 0.4100 acc_test: 0.4190 time: 0.0554s\n",
      "Epoch: 0098 loss_train: 0.5179 acc_train: 0.9500 acc_val: 0.3880 acc_test: 0.3960 time: 0.0556s\n",
      "Epoch: 0099 loss_train: 0.5515 acc_train: 0.9333 acc_val: 0.3760 acc_test: 0.3890 time: 0.0558s\n",
      "Epoch: 0100 loss_train: 0.5589 acc_train: 0.9250 acc_val: 0.3760 acc_test: 0.3990 time: 0.0555s\n",
      "Epoch: 0101 loss_train: 0.5805 acc_train: 0.8917 acc_val: 0.3760 acc_test: 0.4000 time: 0.0561s\n",
      "Epoch: 0102 loss_train: 0.5324 acc_train: 0.9250 acc_val: 0.3780 acc_test: 0.4070 time: 0.0557s\n",
      "Epoch: 0103 loss_train: 0.5954 acc_train: 0.9500 acc_val: 0.3940 acc_test: 0.3980 time: 0.0555s\n",
      "Epoch: 0104 loss_train: 0.5725 acc_train: 0.9583 acc_val: 0.3960 acc_test: 0.3900 time: 0.0558s\n",
      "Epoch: 0105 loss_train: 0.5443 acc_train: 0.9500 acc_val: 0.3760 acc_test: 0.3820 time: 0.0554s\n",
      "Epoch: 0106 loss_train: 0.5596 acc_train: 0.9417 acc_val: 0.3800 acc_test: 0.3840 time: 0.0560s\n",
      "Epoch: 0107 loss_train: 0.5750 acc_train: 0.9333 acc_val: 0.3880 acc_test: 0.3800 time: 0.0555s\n",
      "Epoch: 0108 loss_train: 0.5370 acc_train: 0.9667 acc_val: 0.3780 acc_test: 0.3770 time: 0.0557s\n",
      "Epoch: 0109 loss_train: 0.5418 acc_train: 0.9167 acc_val: 0.3660 acc_test: 0.3760 time: 0.0554s\n",
      "Epoch: 0110 loss_train: 0.6310 acc_train: 0.9083 acc_val: 0.3840 acc_test: 0.4110 time: 0.0553s\n",
      "Epoch: 0111 loss_train: 0.6816 acc_train: 0.9000 acc_val: 0.3420 acc_test: 0.3630 time: 0.0558s\n",
      "Epoch: 0112 loss_train: 0.6601 acc_train: 0.8917 acc_val: 0.3240 acc_test: 0.3360 time: 0.0555s\n",
      "Epoch: 0113 loss_train: 0.4345 acc_train: 0.9583 acc_val: 0.3180 acc_test: 0.3300 time: 0.0559s\n",
      "Epoch: 0114 loss_train: 0.6250 acc_train: 0.9000 acc_val: 0.3060 acc_test: 0.3150 time: 0.0555s\n",
      "Epoch: 0115 loss_train: 0.4459 acc_train: 0.9500 acc_val: 0.3140 acc_test: 0.3390 time: 0.0554s\n",
      "Epoch: 0116 loss_train: 0.6119 acc_train: 0.9000 acc_val: 0.3540 acc_test: 0.3740 time: 0.0559s\n",
      "Epoch: 0117 loss_train: 0.5175 acc_train: 0.9583 acc_val: 0.3960 acc_test: 0.4070 time: 0.0554s\n",
      "Epoch: 0118 loss_train: 0.5628 acc_train: 0.9583 acc_val: 0.4160 acc_test: 0.4250 time: 0.0559s\n",
      "Epoch: 0119 loss_train: 0.5808 acc_train: 0.9500 acc_val: 0.4480 acc_test: 0.4670 time: 0.0554s\n",
      "Epoch: 0120 loss_train: 0.4877 acc_train: 0.9583 acc_val: 0.4180 acc_test: 0.4530 time: 0.0556s\n",
      "Epoch: 0121 loss_train: 0.5234 acc_train: 0.9250 acc_val: 0.3840 acc_test: 0.3820 time: 0.0558s\n",
      "Epoch: 0122 loss_train: 0.5041 acc_train: 0.9667 acc_val: 0.3640 acc_test: 0.3610 time: 0.0555s\n",
      "Epoch: 0123 loss_train: 0.4963 acc_train: 0.9417 acc_val: 0.3580 acc_test: 0.3520 time: 0.0559s\n",
      "Epoch: 0124 loss_train: 0.5293 acc_train: 0.9250 acc_val: 0.3520 acc_test: 0.3480 time: 0.0554s\n",
      "Epoch: 0125 loss_train: 0.4921 acc_train: 0.9667 acc_val: 0.3500 acc_test: 0.3460 time: 0.0557s\n",
      "Epoch: 0126 loss_train: 0.5175 acc_train: 0.9167 acc_val: 0.3640 acc_test: 0.3510 time: 0.0556s\n",
      "Epoch: 0127 loss_train: 0.5844 acc_train: 0.9000 acc_val: 0.3740 acc_test: 0.3800 time: 0.0556s\n",
      "Epoch: 0128 loss_train: 0.5982 acc_train: 0.9667 acc_val: 0.3760 acc_test: 0.3990 time: 0.0558s\n",
      "Epoch: 0129 loss_train: 0.6713 acc_train: 0.8750 acc_val: 0.3780 acc_test: 0.3980 time: 0.0555s\n",
      "Epoch: 0130 loss_train: 0.5901 acc_train: 0.9500 acc_val: 0.3820 acc_test: 0.4010 time: 0.0560s\n",
      "Epoch: 0131 loss_train: 0.4834 acc_train: 0.9250 acc_val: 0.3800 acc_test: 0.4020 time: 0.0556s\n",
      "Epoch: 0132 loss_train: 0.5078 acc_train: 0.9500 acc_val: 0.3740 acc_test: 0.3940 time: 0.0554s\n",
      "Epoch: 0133 loss_train: 0.4783 acc_train: 0.9583 acc_val: 0.3800 acc_test: 0.3960 time: 0.0558s\n",
      "Epoch: 0134 loss_train: 0.5168 acc_train: 0.9750 acc_val: 0.4020 acc_test: 0.4260 time: 0.0556s\n",
      "Epoch: 0135 loss_train: 0.5650 acc_train: 0.9167 acc_val: 0.4220 acc_test: 0.4360 time: 0.0558s\n",
      "Epoch: 0136 loss_train: 0.5298 acc_train: 0.9417 acc_val: 0.4160 acc_test: 0.4400 time: 0.0554s\n",
      "Epoch: 0137 loss_train: 0.5896 acc_train: 0.9083 acc_val: 0.4180 acc_test: 0.4450 time: 0.0555s\n",
      "Epoch: 0138 loss_train: 0.5238 acc_train: 0.9333 acc_val: 0.4180 acc_test: 0.4530 time: 0.0560s\n",
      "Epoch: 0139 loss_train: 0.5707 acc_train: 0.9083 acc_val: 0.3760 acc_test: 0.4110 time: 0.0554s\n",
      "Epoch: 0140 loss_train: 0.4654 acc_train: 0.9583 acc_val: 0.3800 acc_test: 0.4020 time: 0.0556s\n",
      "Epoch: 0141 loss_train: 0.5597 acc_train: 0.9333 acc_val: 0.3860 acc_test: 0.4070 time: 0.0554s\n",
      "Epoch: 0142 loss_train: 0.5628 acc_train: 0.9000 acc_val: 0.3840 acc_test: 0.4100 time: 0.0558s\n",
      "Epoch: 0143 loss_train: 0.5501 acc_train: 0.9250 acc_val: 0.3940 acc_test: 0.4180 time: 0.0557s\n",
      "Epoch: 0144 loss_train: 0.5218 acc_train: 0.9333 acc_val: 0.3800 acc_test: 0.4050 time: 0.0554s\n",
      "Epoch: 0145 loss_train: 0.5881 acc_train: 0.9250 acc_val: 0.3840 acc_test: 0.4040 time: 0.0559s\n",
      "Epoch: 0146 loss_train: 0.5406 acc_train: 0.9583 acc_val: 0.3840 acc_test: 0.4000 time: 0.0556s\n",
      "Epoch: 0147 loss_train: 0.5359 acc_train: 0.9667 acc_val: 0.3900 acc_test: 0.4010 time: 0.0559s\n",
      "Epoch: 0148 loss_train: 0.4974 acc_train: 0.9333 acc_val: 0.3840 acc_test: 0.3900 time: 0.0553s\n",
      "Epoch: 0149 loss_train: 0.5865 acc_train: 0.9583 acc_val: 0.3680 acc_test: 0.3730 time: 0.0555s\n",
      "Epoch: 0150 loss_train: 0.5949 acc_train: 0.9250 acc_val: 0.3540 acc_test: 0.3710 time: 0.0558s\n",
      "Epoch: 0151 loss_train: 0.5718 acc_train: 0.9500 acc_val: 0.3720 acc_test: 0.3950 time: 0.0555s\n",
      "Epoch: 0152 loss_train: 0.5892 acc_train: 0.9333 acc_val: 0.4220 acc_test: 0.4430 time: 0.0558s\n",
      "Epoch: 0153 loss_train: 0.5759 acc_train: 0.9583 acc_val: 0.3780 acc_test: 0.3970 time: 0.0553s\n",
      "Epoch: 0154 loss_train: 0.5711 acc_train: 0.9333 acc_val: 0.3660 acc_test: 0.3790 time: 0.0559s\n",
      "Epoch: 0155 loss_train: 0.5080 acc_train: 0.9417 acc_val: 0.3620 acc_test: 0.3700 time: 0.0558s\n",
      "Epoch: 0156 loss_train: 0.5235 acc_train: 0.8917 acc_val: 0.3640 acc_test: 0.3790 time: 0.0554s\n",
      "Epoch: 0157 loss_train: 0.5834 acc_train: 0.8833 acc_val: 0.3780 acc_test: 0.3940 time: 0.0558s\n",
      "Epoch: 0158 loss_train: 0.4406 acc_train: 0.9500 acc_val: 0.4020 acc_test: 0.4180 time: 0.0556s\n",
      "Epoch: 0159 loss_train: 0.5859 acc_train: 0.9500 acc_val: 0.4060 acc_test: 0.4330 time: 0.0559s\n",
      "Epoch: 0160 loss_train: 0.4853 acc_train: 0.9667 acc_val: 0.3920 acc_test: 0.4260 time: 0.0556s\n",
      "Epoch: 0161 loss_train: 0.5384 acc_train: 0.9333 acc_val: 0.3680 acc_test: 0.3860 time: 0.0555s\n",
      "Epoch: 0162 loss_train: 0.5585 acc_train: 0.9667 acc_val: 0.3660 acc_test: 0.3680 time: 0.0560s\n",
      "Epoch: 0163 loss_train: 0.5643 acc_train: 0.9583 acc_val: 0.3560 acc_test: 0.3640 time: 0.0555s\n",
      "Epoch: 0164 loss_train: 0.5408 acc_train: 0.9167 acc_val: 0.3740 acc_test: 0.3730 time: 0.0559s\n",
      "Epoch: 0165 loss_train: 0.6698 acc_train: 0.9000 acc_val: 0.4300 acc_test: 0.4450 time: 0.0554s\n",
      "Epoch: 0166 loss_train: 0.5113 acc_train: 0.9750 acc_val: 0.4240 acc_test: 0.4370 time: 0.0556s\n",
      "Epoch: 0167 loss_train: 0.5450 acc_train: 0.9333 acc_val: 0.4160 acc_test: 0.4360 time: 0.0559s\n",
      "Epoch: 0168 loss_train: 0.5263 acc_train: 0.9417 acc_val: 0.4100 acc_test: 0.4280 time: 0.0554s\n",
      "Epoch: 0169 loss_train: 0.5714 acc_train: 0.9000 acc_val: 0.4060 acc_test: 0.4320 time: 0.0558s\n",
      "Epoch: 0170 loss_train: 0.5612 acc_train: 0.9333 acc_val: 0.3660 acc_test: 0.3920 time: 0.0557s\n",
      "Epoch: 0171 loss_train: 0.6009 acc_train: 0.9417 acc_val: 0.3620 acc_test: 0.3860 time: 0.0555s\n",
      "Epoch: 0172 loss_train: 0.5941 acc_train: 0.9167 acc_val: 0.3660 acc_test: 0.3920 time: 0.0557s\n",
      "Epoch: 0173 loss_train: 0.6626 acc_train: 0.8833 acc_val: 0.3700 acc_test: 0.3900 time: 0.0555s\n",
      "Epoch: 0174 loss_train: 0.5248 acc_train: 0.9167 acc_val: 0.3920 acc_test: 0.4030 time: 0.0558s\n",
      "Epoch: 0175 loss_train: 0.5052 acc_train: 0.9667 acc_val: 0.3900 acc_test: 0.4100 time: 0.0554s\n",
      "Epoch: 0176 loss_train: 0.5407 acc_train: 0.9417 acc_val: 0.4020 acc_test: 0.4340 time: 0.0556s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0177 loss_train: 0.5511 acc_train: 0.9667 acc_val: 0.3760 acc_test: 0.3910 time: 0.0556s\n",
      "Epoch: 0178 loss_train: 0.6150 acc_train: 0.8833 acc_val: 0.3640 acc_test: 0.3560 time: 0.0555s\n",
      "Epoch: 0179 loss_train: 0.5149 acc_train: 0.9417 acc_val: 0.3660 acc_test: 0.3460 time: 0.0558s\n",
      "Epoch: 0180 loss_train: 0.5065 acc_train: 0.9583 acc_val: 0.3660 acc_test: 0.3480 time: 0.0554s\n",
      "Epoch: 0181 loss_train: 0.6290 acc_train: 0.9167 acc_val: 0.3640 acc_test: 0.3460 time: 0.0558s\n",
      "Epoch: 0182 loss_train: 0.5715 acc_train: 0.9083 acc_val: 0.3680 acc_test: 0.3620 time: 0.0553s\n",
      "Epoch: 0183 loss_train: 0.5042 acc_train: 0.9667 acc_val: 0.3840 acc_test: 0.3920 time: 0.0555s\n",
      "Epoch: 0184 loss_train: 0.5228 acc_train: 0.9667 acc_val: 0.4120 acc_test: 0.4380 time: 0.0558s\n",
      "Epoch: 0185 loss_train: 0.5001 acc_train: 0.9583 acc_val: 0.4320 acc_test: 0.4490 time: 0.0554s\n",
      "Epoch: 0186 loss_train: 0.5856 acc_train: 0.9250 acc_val: 0.4320 acc_test: 0.4550 time: 0.0560s\n",
      "Epoch: 0187 loss_train: 0.4402 acc_train: 0.9583 acc_val: 0.3760 acc_test: 0.3950 time: 0.0555s\n",
      "Epoch: 0188 loss_train: 0.5582 acc_train: 0.9000 acc_val: 0.3680 acc_test: 0.3830 time: 0.0555s\n",
      "Epoch: 0189 loss_train: 0.5571 acc_train: 0.9167 acc_val: 0.3720 acc_test: 0.3820 time: 0.0557s\n",
      "Epoch: 0190 loss_train: 0.6334 acc_train: 0.8750 acc_val: 0.3760 acc_test: 0.3980 time: 0.0556s\n",
      "Epoch: 0191 loss_train: 0.4880 acc_train: 0.9500 acc_val: 0.3840 acc_test: 0.4210 time: 0.0559s\n",
      "Epoch: 0192 loss_train: 0.5819 acc_train: 0.9333 acc_val: 0.4240 acc_test: 0.4700 time: 0.0554s\n",
      "Epoch: 0193 loss_train: 0.4863 acc_train: 0.9667 acc_val: 0.4080 acc_test: 0.4620 time: 0.0558s\n",
      "Epoch: 0194 loss_train: 0.4986 acc_train: 0.9417 acc_val: 0.4160 acc_test: 0.4560 time: 0.0557s\n",
      "Epoch: 0195 loss_train: 0.5604 acc_train: 0.9750 acc_val: 0.4200 acc_test: 0.4460 time: 0.0554s\n",
      "Epoch: 0196 loss_train: 0.5447 acc_train: 0.9333 acc_val: 0.4080 acc_test: 0.4230 time: 0.0558s\n",
      "Epoch: 0197 loss_train: 0.5478 acc_train: 0.9583 acc_val: 0.3940 acc_test: 0.4010 time: 0.0554s\n",
      "Epoch: 0198 loss_train: 0.5589 acc_train: 0.9500 acc_val: 0.3860 acc_test: 0.3920 time: 0.0559s\n",
      "Epoch: 0199 loss_train: 0.5200 acc_train: 0.9750 acc_val: 0.3740 acc_test: 0.3930 time: 0.0557s\n",
      "Epoch: 0200 loss_train: 0.5500 acc_train: 0.9500 acc_val: 0.3680 acc_test: 0.3900 time: 0.0555s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 11.2674s\n",
      "0.448 0.467\n",
      "0.442 0.464\n",
      "0.432 0.455\n",
      "0.43 0.445\n",
      "0.428 0.45\n",
      "0.426 0.444\n",
      "0.424 0.47000000000000003\n",
      "0.422 0.443\n",
      "0.42 0.446\n",
      "0.418 0.453\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN(nfeat=adj.shape[1],\n",
    "        nclass=labels.max().item() + 1,\n",
    "        dropout=0.5)\n",
    "model1.cuda()\n",
    "optimizer = optim.Adam(model1.parameters(),\n",
    "                       lr=0.02, weight_decay=5e-4)\n",
    "t_total = time.time()\n",
    "record = {}\n",
    "for epoch in range(200):\n",
    "    train(epoch,model1,record,features,adj1,adj2,adj3,adj4,adj5)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "bit_list = sorted(record.keys())\n",
    "bit_list.reverse()\n",
    "for key in bit_list[:10]:\n",
    "    value = record[key]\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
